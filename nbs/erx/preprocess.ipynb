{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_tuple(x):\n",
    "    return tuple(sorted(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 experiments\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'commit': 'workspace',\n",
       " 'id': 'workspace',\n",
       " 'name': None,\n",
       " 'params': {'task': 'erx',\n",
       "  'train': {'dataset': {'path': 'bdsaglam/web_nlg-erx-concat',\n",
       "    'name': 'release_v3.0_en',\n",
       "    'split': 'train[:100]'},\n",
       "   'optimizer': 'noop',\n",
       "   'ensemble': 'no'},\n",
       "  'evaluation': {'dataset': {'path': 'bdsaglam/web_nlg-erx-concat',\n",
       "    'name': 'release_v3.0_en',\n",
       "    'split': 'dev[:1000]'}},\n",
       "  'program': {'prompting': 'structured'},\n",
       "  'lm': {'model': 'llama-3-8b', 'temperature': 0.0},\n",
       "  'run': 1},\n",
       " 'metrics': {'exact.precision': 0.014585968840764236,\n",
       "  'exact.recall': 0.013411106866989222,\n",
       "  'exact.f1': 0.013893968089212146,\n",
       "  'fuzzy.precision': 0.2980273675422333,\n",
       "  'fuzzy.recall': 0.2736804602675239,\n",
       "  'fuzzy.f1': 0.28203483319061007}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from adapt.dvc import load_experiments\n",
    "\n",
    "filepaths = list(Path(\"../../tmp/erx/\").glob(\"*.json\"))\n",
    "experiments = [exp for fp in filepaths for exp in load_experiments(fp)]\n",
    "print(f\"{len(experiments)} experiments\")\n",
    "experiments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 experiments before preprocessing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>params.task</th>\n",
       "      <th>params.train.dataset.path</th>\n",
       "      <th>params.train.dataset.name</th>\n",
       "      <th>params.train.dataset.split</th>\n",
       "      <th>params.train.optimizer</th>\n",
       "      <th>params.train.ensemble</th>\n",
       "      <th>params.evaluation.dataset.path</th>\n",
       "      <th>params.evaluation.dataset.name</th>\n",
       "      <th>...</th>\n",
       "      <th>params.program.prompting</th>\n",
       "      <th>params.lm.model</th>\n",
       "      <th>params.lm.temperature</th>\n",
       "      <th>params.run</th>\n",
       "      <th>metrics.exact.precision</th>\n",
       "      <th>metrics.exact.recall</th>\n",
       "      <th>metrics.exact.f1</th>\n",
       "      <th>metrics.fuzzy.precision</th>\n",
       "      <th>metrics.fuzzy.recall</th>\n",
       "      <th>metrics.fuzzy.f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>workspace</td>\n",
       "      <td>None</td>\n",
       "      <td>erx</td>\n",
       "      <td>bdsaglam/web_nlg-erx-concat</td>\n",
       "      <td>release_v3.0_en</td>\n",
       "      <td>train[:100]</td>\n",
       "      <td>noop</td>\n",
       "      <td>no</td>\n",
       "      <td>bdsaglam/web_nlg-erx-concat</td>\n",
       "      <td>release_v3.0_en</td>\n",
       "      <td>...</td>\n",
       "      <td>structured</td>\n",
       "      <td>llama-3-8b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014586</td>\n",
       "      <td>0.013411</td>\n",
       "      <td>0.013894</td>\n",
       "      <td>0.298027</td>\n",
       "      <td>0.273680</td>\n",
       "      <td>0.282035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8a95fd4dc54ea708f0f7421b291c9ec0e1c8f550</td>\n",
       "      <td>lathy-jaws</td>\n",
       "      <td>erx</td>\n",
       "      <td>bdsaglam/web_nlg-erx-concat</td>\n",
       "      <td>release_v3.0_en</td>\n",
       "      <td>train[:100]</td>\n",
       "      <td>noop</td>\n",
       "      <td>no</td>\n",
       "      <td>bdsaglam/web_nlg-erx-concat</td>\n",
       "      <td>release_v3.0_en</td>\n",
       "      <td>...</td>\n",
       "      <td>sft</td>\n",
       "      <td>llama-3-8b-sft</td>\n",
       "      <td>0.0</td>\n",
       "      <td>low</td>\n",
       "      <td>0.630100</td>\n",
       "      <td>0.622543</td>\n",
       "      <td>0.623483</td>\n",
       "      <td>0.850379</td>\n",
       "      <td>0.847531</td>\n",
       "      <td>0.844171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10018c1a534cbb1bb093856fe741782b063885d5</td>\n",
       "      <td>bluff-dita</td>\n",
       "      <td>erx</td>\n",
       "      <td>bdsaglam/web_nlg-erx-concat</td>\n",
       "      <td>release_v3.0_en</td>\n",
       "      <td>train[:100]</td>\n",
       "      <td>bfsrs-high</td>\n",
       "      <td>no</td>\n",
       "      <td>bdsaglam/web_nlg-erx-concat</td>\n",
       "      <td>release_v3.0_en</td>\n",
       "      <td>...</td>\n",
       "      <td>structured</td>\n",
       "      <td>llama-3-8b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.280462</td>\n",
       "      <td>0.275396</td>\n",
       "      <td>0.276618</td>\n",
       "      <td>0.676080</td>\n",
       "      <td>0.664274</td>\n",
       "      <td>0.666510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d3bb54eb8efdc0a2bb5c86befb6b9c5cd0159ebf</td>\n",
       "      <td>irony-gust</td>\n",
       "      <td>erx</td>\n",
       "      <td>bdsaglam/web_nlg-erx-concat</td>\n",
       "      <td>release_v3.0_en</td>\n",
       "      <td>train[:100]</td>\n",
       "      <td>miprov2-medium</td>\n",
       "      <td>no</td>\n",
       "      <td>bdsaglam/web_nlg-erx-concat</td>\n",
       "      <td>release_v3.0_en</td>\n",
       "      <td>...</td>\n",
       "      <td>structured</td>\n",
       "      <td>llama-3-8b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250988</td>\n",
       "      <td>0.233233</td>\n",
       "      <td>0.240662</td>\n",
       "      <td>0.653248</td>\n",
       "      <td>0.607018</td>\n",
       "      <td>0.625459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38621dd5eb7f657d0ed4c3060ec1bfdaab830cda</td>\n",
       "      <td>outer-ring</td>\n",
       "      <td>erx</td>\n",
       "      <td>bdsaglam/web_nlg-erx-concat</td>\n",
       "      <td>release_v3.0_en</td>\n",
       "      <td>train[:100]</td>\n",
       "      <td>bfsrs-medium</td>\n",
       "      <td>no</td>\n",
       "      <td>bdsaglam/web_nlg-erx-concat</td>\n",
       "      <td>release_v3.0_en</td>\n",
       "      <td>...</td>\n",
       "      <td>structured</td>\n",
       "      <td>llama-3-8b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.255153</td>\n",
       "      <td>0.246698</td>\n",
       "      <td>0.249767</td>\n",
       "      <td>0.651775</td>\n",
       "      <td>0.629756</td>\n",
       "      <td>0.637432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id        name params.task  \\\n",
       "0                                 workspace        None         erx   \n",
       "1  8a95fd4dc54ea708f0f7421b291c9ec0e1c8f550  lathy-jaws         erx   \n",
       "2  10018c1a534cbb1bb093856fe741782b063885d5  bluff-dita         erx   \n",
       "3  d3bb54eb8efdc0a2bb5c86befb6b9c5cd0159ebf  irony-gust         erx   \n",
       "4  38621dd5eb7f657d0ed4c3060ec1bfdaab830cda  outer-ring         erx   \n",
       "\n",
       "     params.train.dataset.path params.train.dataset.name  \\\n",
       "0  bdsaglam/web_nlg-erx-concat           release_v3.0_en   \n",
       "1  bdsaglam/web_nlg-erx-concat           release_v3.0_en   \n",
       "2  bdsaglam/web_nlg-erx-concat           release_v3.0_en   \n",
       "3  bdsaglam/web_nlg-erx-concat           release_v3.0_en   \n",
       "4  bdsaglam/web_nlg-erx-concat           release_v3.0_en   \n",
       "\n",
       "  params.train.dataset.split params.train.optimizer params.train.ensemble  \\\n",
       "0                train[:100]                   noop                    no   \n",
       "1                train[:100]                   noop                    no   \n",
       "2                train[:100]             bfsrs-high                    no   \n",
       "3                train[:100]         miprov2-medium                    no   \n",
       "4                train[:100]           bfsrs-medium                    no   \n",
       "\n",
       "  params.evaluation.dataset.path params.evaluation.dataset.name  ...  \\\n",
       "0    bdsaglam/web_nlg-erx-concat                release_v3.0_en  ...   \n",
       "1    bdsaglam/web_nlg-erx-concat                release_v3.0_en  ...   \n",
       "2    bdsaglam/web_nlg-erx-concat                release_v3.0_en  ...   \n",
       "3    bdsaglam/web_nlg-erx-concat                release_v3.0_en  ...   \n",
       "4    bdsaglam/web_nlg-erx-concat                release_v3.0_en  ...   \n",
       "\n",
       "  params.program.prompting params.lm.model params.lm.temperature  params.run  \\\n",
       "0               structured      llama-3-8b                   0.0           1   \n",
       "1                      sft  llama-3-8b-sft                   0.0         low   \n",
       "2               structured      llama-3-8b                   0.0           1   \n",
       "3               structured      llama-3-8b                   0.0           1   \n",
       "4               structured      llama-3-8b                   0.0           1   \n",
       "\n",
       "  metrics.exact.precision  metrics.exact.recall  metrics.exact.f1  \\\n",
       "0                0.014586              0.013411          0.013894   \n",
       "1                0.630100              0.622543          0.623483   \n",
       "2                0.280462              0.275396          0.276618   \n",
       "3                0.250988              0.233233          0.240662   \n",
       "4                0.255153              0.246698          0.249767   \n",
       "\n",
       "   metrics.fuzzy.precision  metrics.fuzzy.recall  metrics.fuzzy.f1  \n",
       "0                 0.298027              0.273680          0.282035  \n",
       "1                 0.850379              0.847531          0.844171  \n",
       "2                 0.676080              0.664274          0.666510  \n",
       "3                 0.653248              0.607018          0.625459  \n",
       "4                 0.651775              0.629756          0.637432  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.json_normalize(experiments).drop(columns=[\"commit\"])\n",
    "print(f\"{len(df)} experiments before preprocessing\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df['name'].isin(['crumb-geum'])\n",
    "df = df.loc[~mask].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_cols = [col for col in df.columns if col.startswith(\"params.\")]\n",
    "metric_cols = [col for col in df.columns if col.startswith(\"metrics.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['params.program.prompting'] = df['params.program.prompting'].fillna('structured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 experiments after preprocessing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>params.task</th>\n",
       "      <th>params.train.dataset.path</th>\n",
       "      <th>params.train.dataset.name</th>\n",
       "      <th>params.train.dataset.split</th>\n",
       "      <th>params.train.optimizer</th>\n",
       "      <th>params.train.ensemble</th>\n",
       "      <th>params.evaluation.dataset.path</th>\n",
       "      <th>params.evaluation.dataset.name</th>\n",
       "      <th>...</th>\n",
       "      <th>params.program.prompting</th>\n",
       "      <th>params.lm.model</th>\n",
       "      <th>params.lm.temperature</th>\n",
       "      <th>params.run</th>\n",
       "      <th>metrics.exact.precision</th>\n",
       "      <th>metrics.exact.recall</th>\n",
       "      <th>metrics.exact.f1</th>\n",
       "      <th>metrics.fuzzy.precision</th>\n",
       "      <th>metrics.fuzzy.recall</th>\n",
       "      <th>metrics.fuzzy.f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8a95fd4dc54ea708f0f7421b291c9ec0e1c8f550</td>\n",
       "      <td>lathy-jaws</td>\n",
       "      <td>erx</td>\n",
       "      <td>bdsaglam/web_nlg-erx-concat</td>\n",
       "      <td>release_v3.0_en</td>\n",
       "      <td>train[:100]</td>\n",
       "      <td>noop</td>\n",
       "      <td>no</td>\n",
       "      <td>bdsaglam/web_nlg-erx-concat</td>\n",
       "      <td>release_v3.0_en</td>\n",
       "      <td>...</td>\n",
       "      <td>sft</td>\n",
       "      <td>llama-3-8b-sft</td>\n",
       "      <td>0.0</td>\n",
       "      <td>low</td>\n",
       "      <td>0.630100</td>\n",
       "      <td>0.622543</td>\n",
       "      <td>0.623483</td>\n",
       "      <td>0.850379</td>\n",
       "      <td>0.847531</td>\n",
       "      <td>0.844171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>207289e5a394e3f9ae82cf08aea3d2ca07e190ba</td>\n",
       "      <td>moral-prof</td>\n",
       "      <td>erx</td>\n",
       "      <td>bdsaglam/web_nlg-erx-concat</td>\n",
       "      <td>release_v3.0_en</td>\n",
       "      <td>train[:100]</td>\n",
       "      <td>noop</td>\n",
       "      <td>no</td>\n",
       "      <td>bdsaglam/web_nlg-erx-concat</td>\n",
       "      <td>release_v3.0_en</td>\n",
       "      <td>...</td>\n",
       "      <td>sft</td>\n",
       "      <td>llama-3-8b-sft</td>\n",
       "      <td>0.0</td>\n",
       "      <td>high</td>\n",
       "      <td>0.945660</td>\n",
       "      <td>0.939192</td>\n",
       "      <td>0.941871</td>\n",
       "      <td>0.980038</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.976060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>e20f45a313f10b09f83c05fdb44f275d52da7799</td>\n",
       "      <td>spicy-teff</td>\n",
       "      <td>erx</td>\n",
       "      <td>bdsaglam/web_nlg-erx-concat</td>\n",
       "      <td>release_v3.0_en</td>\n",
       "      <td>train[:100]</td>\n",
       "      <td>noop</td>\n",
       "      <td>no</td>\n",
       "      <td>bdsaglam/web_nlg-erx-concat</td>\n",
       "      <td>release_v3.0_en</td>\n",
       "      <td>...</td>\n",
       "      <td>sft</td>\n",
       "      <td>llama-3-8b-sft</td>\n",
       "      <td>0.0</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.826799</td>\n",
       "      <td>0.806647</td>\n",
       "      <td>0.814956</td>\n",
       "      <td>0.942006</td>\n",
       "      <td>0.920719</td>\n",
       "      <td>0.929080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>e83c0e826d873aca30095d400da7db869e742d0a</td>\n",
       "      <td>gawsy-paps</td>\n",
       "      <td>erx</td>\n",
       "      <td>bdsaglam/web_nlg-erx-concat</td>\n",
       "      <td>release_v3.0_en</td>\n",
       "      <td>train[:100]</td>\n",
       "      <td>bfsrs-ulti</td>\n",
       "      <td>no</td>\n",
       "      <td>bdsaglam/web_nlg-erx-concat</td>\n",
       "      <td>release_v3.0_en</td>\n",
       "      <td>...</td>\n",
       "      <td>structured</td>\n",
       "      <td>llama-3-8b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.320626</td>\n",
       "      <td>0.319126</td>\n",
       "      <td>0.318514</td>\n",
       "      <td>0.698233</td>\n",
       "      <td>0.695948</td>\n",
       "      <td>0.693846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6f84f2616434f0cb73f78dd99349821288535df6</td>\n",
       "      <td>metal-pons</td>\n",
       "      <td>erx</td>\n",
       "      <td>bdsaglam/web_nlg-erx-concat</td>\n",
       "      <td>release_v3.0_en</td>\n",
       "      <td>train[:100]</td>\n",
       "      <td>bfsrs-medium</td>\n",
       "      <td>no</td>\n",
       "      <td>bdsaglam/web_nlg-erx-concat</td>\n",
       "      <td>release_v3.0_en</td>\n",
       "      <td>...</td>\n",
       "      <td>structured</td>\n",
       "      <td>qwen-2.5-32b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.377839</td>\n",
       "      <td>0.367084</td>\n",
       "      <td>0.370645</td>\n",
       "      <td>0.797541</td>\n",
       "      <td>0.779191</td>\n",
       "      <td>0.783746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          id        name params.task  \\\n",
       "1   8a95fd4dc54ea708f0f7421b291c9ec0e1c8f550  lathy-jaws         erx   \n",
       "13  207289e5a394e3f9ae82cf08aea3d2ca07e190ba  moral-prof         erx   \n",
       "14  e20f45a313f10b09f83c05fdb44f275d52da7799  spicy-teff         erx   \n",
       "18  e83c0e826d873aca30095d400da7db869e742d0a  gawsy-paps         erx   \n",
       "21  6f84f2616434f0cb73f78dd99349821288535df6  metal-pons         erx   \n",
       "\n",
       "      params.train.dataset.path params.train.dataset.name  \\\n",
       "1   bdsaglam/web_nlg-erx-concat           release_v3.0_en   \n",
       "13  bdsaglam/web_nlg-erx-concat           release_v3.0_en   \n",
       "14  bdsaglam/web_nlg-erx-concat           release_v3.0_en   \n",
       "18  bdsaglam/web_nlg-erx-concat           release_v3.0_en   \n",
       "21  bdsaglam/web_nlg-erx-concat           release_v3.0_en   \n",
       "\n",
       "   params.train.dataset.split params.train.optimizer params.train.ensemble  \\\n",
       "1                 train[:100]                   noop                    no   \n",
       "13                train[:100]                   noop                    no   \n",
       "14                train[:100]                   noop                    no   \n",
       "18                train[:100]             bfsrs-ulti                    no   \n",
       "21                train[:100]           bfsrs-medium                    no   \n",
       "\n",
       "   params.evaluation.dataset.path params.evaluation.dataset.name  ...  \\\n",
       "1     bdsaglam/web_nlg-erx-concat                release_v3.0_en  ...   \n",
       "13    bdsaglam/web_nlg-erx-concat                release_v3.0_en  ...   \n",
       "14    bdsaglam/web_nlg-erx-concat                release_v3.0_en  ...   \n",
       "18    bdsaglam/web_nlg-erx-concat                release_v3.0_en  ...   \n",
       "21    bdsaglam/web_nlg-erx-concat                release_v3.0_en  ...   \n",
       "\n",
       "   params.program.prompting params.lm.model params.lm.temperature  params.run  \\\n",
       "1                       sft  llama-3-8b-sft                   0.0         low   \n",
       "13                      sft  llama-3-8b-sft                   0.0        high   \n",
       "14                      sft  llama-3-8b-sft                   0.0      medium   \n",
       "18               structured      llama-3-8b                   0.0           1   \n",
       "21               structured    qwen-2.5-32b                   0.0           1   \n",
       "\n",
       "   metrics.exact.precision  metrics.exact.recall  metrics.exact.f1  \\\n",
       "1                 0.630100              0.622543          0.623483   \n",
       "13                0.945660              0.939192          0.941871   \n",
       "14                0.826799              0.806647          0.814956   \n",
       "18                0.320626              0.319126          0.318514   \n",
       "21                0.377839              0.367084          0.370645   \n",
       "\n",
       "    metrics.fuzzy.precision  metrics.fuzzy.recall  metrics.fuzzy.f1  \n",
       "1                  0.850379              0.847531          0.844171  \n",
       "13                 0.980038              0.973275          0.976060  \n",
       "14                 0.942006              0.920719          0.929080  \n",
       "18                 0.698233              0.695948          0.693846  \n",
       "21                 0.797541              0.779191          0.783746  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(subset=param_cols + metric_cols, inplace=True, how=\"any\")\n",
    "df.drop_duplicates(subset=param_cols, inplace=True, keep='last')\n",
    "\n",
    "print(f\"{len(df)} experiments after preprocessing\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- params.task: ['erx']\n",
      "\n",
      "- params.train.dataset.path: ['bdsaglam/web_nlg-erx-concat']\n",
      "\n",
      "- params.train.dataset.name: ['release_v3.0_en']\n",
      "\n",
      "- params.train.dataset.split: ['train[:100]']\n",
      "\n",
      "- params.train.optimizer: ['noop', 'bfsrs-ulti', 'bfsrs-medium', 'miprov2-light', 'miprov2-medium', 'bfsrs-high']\n",
      "\n",
      "- params.train.ensemble: ['no']\n",
      "\n",
      "- params.evaluation.dataset.path: ['bdsaglam/web_nlg-erx-concat']\n",
      "\n",
      "- params.evaluation.dataset.name: ['release_v3.0_en']\n",
      "\n",
      "- params.evaluation.dataset.split: ['dev', 'dev[:1000]', 'dev[:100]']\n",
      "\n",
      "- params.program.prompting: ['sft', 'structured']\n",
      "\n",
      "- params.lm.model: ['llama-3-8b-sft', 'llama-3-8b', 'qwen-2.5-32b']\n",
      "\n",
      "- params.lm.temperature: [np.float64(0.0), np.float64(0.5)]\n",
      "\n",
      "- params.run: ['low', 'high', 'medium', 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in param_cols:\n",
    "    values = list(df[col].unique())\n",
    "    print(f\"- {col}: {values}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('exps.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup remaining experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_experiment_configs(common_params, varying_params):\n",
    "    # Generate all possible combinations of parameters\n",
    "    varying_params = {**common_params, **varying_params}\n",
    "    keys = varying_params.keys()\n",
    "    values = varying_params.values()\n",
    "    for instance in itertools.product(*values):\n",
    "        yield dict(zip(keys, instance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_all_experiment_configs(common_params: dict, varying_params_list: list[dict]):\n",
    "    for params in varying_params_list:\n",
    "        for exp_config in produce_experiment_configs(common_params, params):\n",
    "            yield exp_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_params = {\n",
    "    \"params.task\": [\"erx\"],\n",
    "    \"params.train.dataset.path\": [\"bdsaglam/web_nlg-erx-concat\"],\n",
    "    \"params.train.dataset.name\": [\"release_v3.0_en\"],\n",
    "    \"params.train.dataset.split\": ['\"train[:100]\"'],\n",
    "    \"params.evaluation.dataset.path\": [\"bdsaglam/web_nlg-erx-concat\"],\n",
    "    \"params.evaluation.dataset.name\": [\"release_v3.0_en\"],\n",
    "    \"params.evaluation.dataset.split\": ['\"dev\"'],\n",
    "    \"params.train.ensemble\": [\n",
    "        \"no\",\n",
    "        # \"yes\",\n",
    "    ],\n",
    "    \"params.lm.temperature\": [\n",
    "        0.0,\n",
    "        # 0.5,\n",
    "        # 0.7,\n",
    "    ],\n",
    "    \"params.run\": [\n",
    "        1,\n",
    "        # 2,\n",
    "        # 3,\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "varying_params_list = [\n",
    "    {\n",
    "        \"params.train.optimizer\": [\n",
    "            \"noop\",\n",
    "            \"bfsrs-medium\",\n",
    "            \"bfsrs-high\",\n",
    "            'bfsrs-ulti',\n",
    "            \"miprov2-light\",\n",
    "            \"miprov2-medium\",\n",
    "        ],\n",
    "        \"params.program.prompting\": [\"structured\"],\n",
    "        \"params.lm.model\": [\n",
    "            # \"llama-3-8b\",\n",
    "            # \"qwen-2.5-32b\",\n",
    "            \"llama-3.3-70b\",\n",
    "        ],\n",
    "    },\n",
    "    # {\n",
    "    #     \"params.train.optimizer\": [\"noop\"],\n",
    "    #     \"params.program.prompting\": [\"sft\"],\n",
    "    #     \"params.lm.model\": [\n",
    "    #         \"llama-3-8b-sft\",\n",
    "    #     ],\n",
    "    # },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 experiment configurations\n",
      "['params.task', 'params.train.dataset.path', 'params.train.dataset.name', 'params.train.dataset.split', 'params.evaluation.dataset.path', 'params.evaluation.dataset.name', 'params.evaluation.dataset.split', 'params.train.ensemble', 'params.lm.temperature', 'params.run', 'params.train.optimizer', 'params.program.prompting', 'params.lm.model']\n"
     ]
    }
   ],
   "source": [
    "exp_configs = list(produce_all_experiment_configs(common_params, varying_params_list))\n",
    "target_params = list(exp_configs[0].keys())\n",
    "print(f\"{len(exp_configs)} experiment configurations\")\n",
    "print(target_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing exps: 24\n"
     ]
    }
   ],
   "source": [
    "if len(df):\n",
    "    existing_configs = df[target_params].to_dict(orient=\"records\")\n",
    "    existing_configs[0]\n",
    "else:\n",
    "    existing_configs = []\n",
    "\n",
    "print(\"Existing exps:\", len(existing_configs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 missing configurations\n"
     ]
    }
   ],
   "source": [
    "# find the missing configurations\n",
    "missing_configs = [\n",
    "    dict(kv)\n",
    "    for kv in list(\n",
    "        {tuple(sorted(config.items())) for config in exp_configs}\n",
    "        - {tuple(sorted(config.items())) for config in existing_configs}\n",
    "    )\n",
    "]\n",
    "print(f\"{len(missing_configs)} missing configurations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_command(exp_config):\n",
    "    lines = [\"dvc exp run --queue\"]\n",
    "    for target_param in target_params:\n",
    "        arg_name = target_param.split(\".\", 1)[-1]\n",
    "        arg_value = exp_config[target_param]\n",
    "        lines.append(f\"-S {arg_name}='{arg_value}'\")\n",
    "\n",
    "    command = \" \\\\\\n    \".join(lines)\n",
    "    return command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"run.sh\", \"w\") as f:\n",
    "    f.write(\"#!/bin/sh\\n\\n\")\n",
    "    for exp_config in missing_configs:\n",
    "        f.write(make_command(exp_config))\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>params.program.prompting</th>\n",
       "      <th>params.run</th>\n",
       "      <th>params.train.optimizer</th>\n",
       "      <th>params.lm.model</th>\n",
       "      <th>params.lm.temperature</th>\n",
       "      <th>metrics.exact.precision</th>\n",
       "      <th>metrics.exact.recall</th>\n",
       "      <th>metrics.exact.f1</th>\n",
       "      <th>metrics.fuzzy.precision</th>\n",
       "      <th>metrics.fuzzy.recall</th>\n",
       "      <th>metrics.fuzzy.f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>moral-prof</td>\n",
       "      <td>sft</td>\n",
       "      <td>high</td>\n",
       "      <td>noop</td>\n",
       "      <td>llama-3-8b-sft</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.945660</td>\n",
       "      <td>0.939192</td>\n",
       "      <td>0.941871</td>\n",
       "      <td>0.980038</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.976060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>spicy-teff</td>\n",
       "      <td>sft</td>\n",
       "      <td>medium</td>\n",
       "      <td>noop</td>\n",
       "      <td>llama-3-8b-sft</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.826799</td>\n",
       "      <td>0.806647</td>\n",
       "      <td>0.814956</td>\n",
       "      <td>0.942006</td>\n",
       "      <td>0.920719</td>\n",
       "      <td>0.929080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lathy-jaws</td>\n",
       "      <td>sft</td>\n",
       "      <td>low</td>\n",
       "      <td>noop</td>\n",
       "      <td>llama-3-8b-sft</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.630100</td>\n",
       "      <td>0.622543</td>\n",
       "      <td>0.623483</td>\n",
       "      <td>0.850379</td>\n",
       "      <td>0.847531</td>\n",
       "      <td>0.844171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>alone-mesh</td>\n",
       "      <td>structured</td>\n",
       "      <td>1</td>\n",
       "      <td>bfsrs-medium</td>\n",
       "      <td>qwen-2.5-32b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.366102</td>\n",
       "      <td>0.358353</td>\n",
       "      <td>0.361248</td>\n",
       "      <td>0.793954</td>\n",
       "      <td>0.783383</td>\n",
       "      <td>0.786429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>metal-pons</td>\n",
       "      <td>structured</td>\n",
       "      <td>1</td>\n",
       "      <td>bfsrs-medium</td>\n",
       "      <td>qwen-2.5-32b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.377839</td>\n",
       "      <td>0.367084</td>\n",
       "      <td>0.370645</td>\n",
       "      <td>0.797541</td>\n",
       "      <td>0.779191</td>\n",
       "      <td>0.783746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gawsy-paps</td>\n",
       "      <td>structured</td>\n",
       "      <td>1</td>\n",
       "      <td>bfsrs-ulti</td>\n",
       "      <td>llama-3-8b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320626</td>\n",
       "      <td>0.319126</td>\n",
       "      <td>0.318514</td>\n",
       "      <td>0.698233</td>\n",
       "      <td>0.695948</td>\n",
       "      <td>0.693846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>slack-poss</td>\n",
       "      <td>structured</td>\n",
       "      <td>1</td>\n",
       "      <td>bfsrs-medium</td>\n",
       "      <td>llama-3-8b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.270941</td>\n",
       "      <td>0.263430</td>\n",
       "      <td>0.266274</td>\n",
       "      <td>0.686891</td>\n",
       "      <td>0.661890</td>\n",
       "      <td>0.671487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>veiny-mina</td>\n",
       "      <td>structured</td>\n",
       "      <td>1</td>\n",
       "      <td>bfsrs-high</td>\n",
       "      <td>llama-3-8b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.280462</td>\n",
       "      <td>0.275396</td>\n",
       "      <td>0.276618</td>\n",
       "      <td>0.676080</td>\n",
       "      <td>0.664274</td>\n",
       "      <td>0.666510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>modal-huck</td>\n",
       "      <td>structured</td>\n",
       "      <td>1</td>\n",
       "      <td>bfsrs-high</td>\n",
       "      <td>llama-3-8b</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.274224</td>\n",
       "      <td>0.268814</td>\n",
       "      <td>0.269906</td>\n",
       "      <td>0.666549</td>\n",
       "      <td>0.656943</td>\n",
       "      <td>0.657637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>unlet-hull</td>\n",
       "      <td>structured</td>\n",
       "      <td>1</td>\n",
       "      <td>bfsrs-medium</td>\n",
       "      <td>llama-3-8b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.255153</td>\n",
       "      <td>0.246698</td>\n",
       "      <td>0.249767</td>\n",
       "      <td>0.651775</td>\n",
       "      <td>0.629756</td>\n",
       "      <td>0.637432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>heigh-mope</td>\n",
       "      <td>structured</td>\n",
       "      <td>1</td>\n",
       "      <td>bfsrs-medium</td>\n",
       "      <td>llama-3-8b</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.251933</td>\n",
       "      <td>0.243802</td>\n",
       "      <td>0.246764</td>\n",
       "      <td>0.650937</td>\n",
       "      <td>0.628296</td>\n",
       "      <td>0.636171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>mired-bats</td>\n",
       "      <td>structured</td>\n",
       "      <td>1</td>\n",
       "      <td>miprov2-medium</td>\n",
       "      <td>llama-3-8b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250988</td>\n",
       "      <td>0.233233</td>\n",
       "      <td>0.240662</td>\n",
       "      <td>0.653248</td>\n",
       "      <td>0.607018</td>\n",
       "      <td>0.625459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>kooky-choc</td>\n",
       "      <td>structured</td>\n",
       "      <td>1</td>\n",
       "      <td>miprov2-medium</td>\n",
       "      <td>llama-3-8b</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.230381</td>\n",
       "      <td>0.214486</td>\n",
       "      <td>0.220995</td>\n",
       "      <td>0.643391</td>\n",
       "      <td>0.597861</td>\n",
       "      <td>0.615783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>zippy-cane</td>\n",
       "      <td>structured</td>\n",
       "      <td>1</td>\n",
       "      <td>miprov2-light</td>\n",
       "      <td>llama-3-8b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133240</td>\n",
       "      <td>0.133156</td>\n",
       "      <td>0.132834</td>\n",
       "      <td>0.588515</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>0.588394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>soupy-beak</td>\n",
       "      <td>structured</td>\n",
       "      <td>1</td>\n",
       "      <td>miprov2-light</td>\n",
       "      <td>llama-3-8b</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.156149</td>\n",
       "      <td>0.156003</td>\n",
       "      <td>0.155215</td>\n",
       "      <td>0.562983</td>\n",
       "      <td>0.569753</td>\n",
       "      <td>0.562845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>shoal-dolt</td>\n",
       "      <td>structured</td>\n",
       "      <td>1</td>\n",
       "      <td>miprov2-light</td>\n",
       "      <td>llama-3-8b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.164665</td>\n",
       "      <td>0.159790</td>\n",
       "      <td>0.161087</td>\n",
       "      <td>0.549267</td>\n",
       "      <td>0.538236</td>\n",
       "      <td>0.540178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>group-nuke</td>\n",
       "      <td>structured</td>\n",
       "      <td>1</td>\n",
       "      <td>miprov2-light</td>\n",
       "      <td>qwen-2.5-32b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064546</td>\n",
       "      <td>0.064266</td>\n",
       "      <td>0.064222</td>\n",
       "      <td>0.530839</td>\n",
       "      <td>0.524796</td>\n",
       "      <td>0.526424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>myoid-kefs</td>\n",
       "      <td>structured</td>\n",
       "      <td>1</td>\n",
       "      <td>miprov2-light</td>\n",
       "      <td>qwen-2.5-32b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072078</td>\n",
       "      <td>0.069922</td>\n",
       "      <td>0.070684</td>\n",
       "      <td>0.532639</td>\n",
       "      <td>0.523590</td>\n",
       "      <td>0.525673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>sated-pond</td>\n",
       "      <td>structured</td>\n",
       "      <td>1</td>\n",
       "      <td>noop</td>\n",
       "      <td>qwen-2.5-32b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020862</td>\n",
       "      <td>0.020312</td>\n",
       "      <td>0.020562</td>\n",
       "      <td>0.455415</td>\n",
       "      <td>0.439280</td>\n",
       "      <td>0.445118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>butch-keek</td>\n",
       "      <td>structured</td>\n",
       "      <td>1</td>\n",
       "      <td>noop</td>\n",
       "      <td>qwen-2.5-32b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016973</td>\n",
       "      <td>0.016721</td>\n",
       "      <td>0.016769</td>\n",
       "      <td>0.447680</td>\n",
       "      <td>0.430505</td>\n",
       "      <td>0.436040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>minim-rods</td>\n",
       "      <td>structured</td>\n",
       "      <td>1</td>\n",
       "      <td>noop</td>\n",
       "      <td>llama-3-8b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018212</td>\n",
       "      <td>0.017030</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.356735</td>\n",
       "      <td>0.335013</td>\n",
       "      <td>0.341740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>sonic-doll</td>\n",
       "      <td>structured</td>\n",
       "      <td>1</td>\n",
       "      <td>noop</td>\n",
       "      <td>llama-3-8b</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.018026</td>\n",
       "      <td>0.016793</td>\n",
       "      <td>0.017272</td>\n",
       "      <td>0.355650</td>\n",
       "      <td>0.339312</td>\n",
       "      <td>0.340013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>None</td>\n",
       "      <td>structured</td>\n",
       "      <td>1</td>\n",
       "      <td>noop</td>\n",
       "      <td>llama-3-8b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014586</td>\n",
       "      <td>0.013411</td>\n",
       "      <td>0.013894</td>\n",
       "      <td>0.298027</td>\n",
       "      <td>0.273680</td>\n",
       "      <td>0.282035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>None</td>\n",
       "      <td>structured</td>\n",
       "      <td>1</td>\n",
       "      <td>noop</td>\n",
       "      <td>llama-3-8b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014586</td>\n",
       "      <td>0.013411</td>\n",
       "      <td>0.013894</td>\n",
       "      <td>0.298027</td>\n",
       "      <td>0.273680</td>\n",
       "      <td>0.282035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name params.program.prompting params.run params.train.optimizer  \\\n",
       "13  moral-prof                      sft       high                   noop   \n",
       "14  spicy-teff                      sft     medium                   noop   \n",
       "1   lathy-jaws                      sft        low                   noop   \n",
       "45  alone-mesh               structured          1           bfsrs-medium   \n",
       "21  metal-pons               structured          1           bfsrs-medium   \n",
       "18  gawsy-paps               structured          1             bfsrs-ulti   \n",
       "46  slack-poss               structured          1           bfsrs-medium   \n",
       "41  veiny-mina               structured          1             bfsrs-high   \n",
       "30  modal-huck               structured          1             bfsrs-high   \n",
       "39  unlet-hull               structured          1           bfsrs-medium   \n",
       "28  heigh-mope               structured          1           bfsrs-medium   \n",
       "42  mired-bats               structured          1         miprov2-medium   \n",
       "29  kooky-choc               structured          1         miprov2-medium   \n",
       "48  zippy-cane               structured          1          miprov2-light   \n",
       "31  soupy-beak               structured          1          miprov2-light   \n",
       "43  shoal-dolt               structured          1          miprov2-light   \n",
       "47  group-nuke               structured          1          miprov2-light   \n",
       "23  myoid-kefs               structured          1          miprov2-light   \n",
       "49  sated-pond               structured          1                   noop   \n",
       "24  butch-keek               structured          1                   noop   \n",
       "40  minim-rods               structured          1                   noop   \n",
       "33  sonic-doll               structured          1                   noop   \n",
       "38        None               structured          1                   noop   \n",
       "44        None               structured          1                   noop   \n",
       "\n",
       "   params.lm.model  params.lm.temperature  metrics.exact.precision  \\\n",
       "13  llama-3-8b-sft                    0.0                 0.945660   \n",
       "14  llama-3-8b-sft                    0.0                 0.826799   \n",
       "1   llama-3-8b-sft                    0.0                 0.630100   \n",
       "45    qwen-2.5-32b                    0.0                 0.366102   \n",
       "21    qwen-2.5-32b                    0.0                 0.377839   \n",
       "18      llama-3-8b                    0.0                 0.320626   \n",
       "46      llama-3-8b                    0.0                 0.270941   \n",
       "41      llama-3-8b                    0.0                 0.280462   \n",
       "30      llama-3-8b                    0.5                 0.274224   \n",
       "39      llama-3-8b                    0.0                 0.255153   \n",
       "28      llama-3-8b                    0.5                 0.251933   \n",
       "42      llama-3-8b                    0.0                 0.250988   \n",
       "29      llama-3-8b                    0.5                 0.230381   \n",
       "48      llama-3-8b                    0.0                 0.133240   \n",
       "31      llama-3-8b                    0.5                 0.156149   \n",
       "43      llama-3-8b                    0.0                 0.164665   \n",
       "47    qwen-2.5-32b                    0.0                 0.064546   \n",
       "23    qwen-2.5-32b                    0.0                 0.072078   \n",
       "49    qwen-2.5-32b                    0.0                 0.020862   \n",
       "24    qwen-2.5-32b                    0.0                 0.016973   \n",
       "40      llama-3-8b                    0.0                 0.018212   \n",
       "33      llama-3-8b                    0.5                 0.018026   \n",
       "38      llama-3-8b                    0.0                 0.014586   \n",
       "44      llama-3-8b                    0.0                 0.014586   \n",
       "\n",
       "    metrics.exact.recall  metrics.exact.f1  metrics.fuzzy.precision  \\\n",
       "13              0.939192          0.941871                 0.980038   \n",
       "14              0.806647          0.814956                 0.942006   \n",
       "1               0.622543          0.623483                 0.850379   \n",
       "45              0.358353          0.361248                 0.793954   \n",
       "21              0.367084          0.370645                 0.797541   \n",
       "18              0.319126          0.318514                 0.698233   \n",
       "46              0.263430          0.266274                 0.686891   \n",
       "41              0.275396          0.276618                 0.676080   \n",
       "30              0.268814          0.269906                 0.666549   \n",
       "39              0.246698          0.249767                 0.651775   \n",
       "28              0.243802          0.246764                 0.650937   \n",
       "42              0.233233          0.240662                 0.653248   \n",
       "29              0.214486          0.220995                 0.643391   \n",
       "48              0.133156          0.132834                 0.588515   \n",
       "31              0.156003          0.155215                 0.562983   \n",
       "43              0.159790          0.161087                 0.549267   \n",
       "47              0.064266          0.064222                 0.530839   \n",
       "23              0.069922          0.070684                 0.532639   \n",
       "49              0.020312          0.020562                 0.455415   \n",
       "24              0.016721          0.016769                 0.447680   \n",
       "40              0.017030          0.017500                 0.356735   \n",
       "33              0.016793          0.017272                 0.355650   \n",
       "38              0.013411          0.013894                 0.298027   \n",
       "44              0.013411          0.013894                 0.298027   \n",
       "\n",
       "    metrics.fuzzy.recall  metrics.fuzzy.f1  \n",
       "13              0.973275          0.976060  \n",
       "14              0.920719          0.929080  \n",
       "1               0.847531          0.844171  \n",
       "45              0.783383          0.786429  \n",
       "21              0.779191          0.783746  \n",
       "18              0.695948          0.693846  \n",
       "46              0.661890          0.671487  \n",
       "41              0.664274          0.666510  \n",
       "30              0.656943          0.657637  \n",
       "39              0.629756          0.637432  \n",
       "28              0.628296          0.636171  \n",
       "42              0.607018          0.625459  \n",
       "29              0.597861          0.615783  \n",
       "48              0.593496          0.588394  \n",
       "31              0.569753          0.562845  \n",
       "43              0.538236          0.540178  \n",
       "47              0.524796          0.526424  \n",
       "23              0.523590          0.525673  \n",
       "49              0.439280          0.445118  \n",
       "24              0.430505          0.436040  \n",
       "40              0.335013          0.341740  \n",
       "33              0.339312          0.340013  \n",
       "38              0.273680          0.282035  \n",
       "44              0.273680          0.282035  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['name', 'params.program.prompting', 'params.run', 'params.train.optimizer', 'params.lm.model', 'params.lm.temperature', *metric_cols]].sort_values(by='metrics.fuzzy.f1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
