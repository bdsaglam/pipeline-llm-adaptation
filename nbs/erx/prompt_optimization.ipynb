{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import dspy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import weave\n",
    "from datasets import load_dataset\n",
    "from dspy.evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://0.0.0.0:8081/v1\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"OPENAI_BASE_URL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed % (2**32 - 1))\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def configure_lm(model, temperature):\n",
    "    lm = dspy.LM(\n",
    "        \"openai/\" + model,\n",
    "        temperature=temperature,\n",
    "        cache=False,\n",
    "        api_base=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    )\n",
    "    dspy.configure(lm=lm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weave.init(project_name=\"llm-adaptation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(89)\n",
    "\n",
    "configure_lm('llama-3-8b', 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "def compute_generalized_scores(\n",
    "    pred_triples: list[str], \n",
    "    reference_triples: list[str], \n",
    "    match_function: Callable[[str, str], bool]\n",
    "):\n",
    "    \"\"\"Compute precision, recall, and F1-score using a customizable match function.\"\"\"\n",
    "\n",
    "    pred_set = set(pred_triples)\n",
    "    reference_set = set(reference_triples)\n",
    "\n",
    "    if not pred_set and not reference_set:\n",
    "        return {\"precision\": 1.0, \"recall\": 1.0, \"f1\": 1.0}\n",
    "\n",
    "    if not pred_set or not reference_set:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "\n",
    "    # Count true positives using the provided match function\n",
    "    true_positives = sum(any(match_function(pred, ref) for ref in reference_set) for pred in pred_set)\n",
    "    \n",
    "    precision = true_positives / len(pred_set)\n",
    "    recall = true_positives / len(reference_set)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "def fuzzy_match(pred: str, ref: str) -> bool:\n",
    "    return fuzz.ratio(pred, ref) > 80\n",
    "\n",
    "def compute_scores(pred_triples: list[str], reference_triples: list[str]):\n",
    "    exact_scores = {f\"exact.{k}\": v for k, v in  compute_generalized_scores(pred_triples, reference_triples, lambda x, y: x == y).items()}\n",
    "    fuzzy_scores = {f\"fuzzy.{k}\": v for k, v in  compute_generalized_scores(pred_triples, reference_triples, fuzzy_match).items()}\n",
    "    return {**exact_scores, **fuzzy_scores}\n",
    "\n",
    "def parse_triples(triples_str: str):\n",
    "    return [triple.strip() for triple in triples_str.split('\\n') if triple.strip()]\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def evaluate_triples(example, pred, trace=None):\n",
    "    return compute_scores(parse_triples(pred.triples_str), example.triples)['fuzzy.f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def dynamic_import(module, name):\n",
    "    import importlib\n",
    "\n",
    "    return getattr(importlib.import_module(module), name)\n",
    "\n",
    "def make_optimizer(optimizer_config: dict):\n",
    "    cls = dynamic_import(\"dspy.teleprompt\", optimizer_config[\"class\"])\n",
    "    kwargs = deepcopy(optimizer_config[\"params\"])\n",
    "    if optimizer_config[\"with_metric\"]:\n",
    "        kwargs[\"metric\"] = evaluate_triples\n",
    "    return cls(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityRelationExtraction(dspy.Signature):\n",
    "    \"\"\"Extract `subject | predicate | object` triples from text.\"\"\"\n",
    "\n",
    "    text: str = dspy.InputField()\n",
    "    triples_str: str = dspy.OutputField(\n",
    "        desc='The triples extracted from the text. Each triple should be in the format \"subject | predicate | object\". Triples should be separated by newlines.'\n",
    "    )\n",
    "\n",
    "\n",
    "def make_program():\n",
    "    return dspy.Predict(EntityRelationExtraction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIError",
     "evalue": "litellm.APIError: APIError: OpenAIException - Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReadError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:101\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n\u001b[1;32m    252\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[39mraise\u001b[39;00m exc \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[39m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mhandle_request(\n\u001b[1;32m    237\u001b[0m         pool_request\u001b[39m.\u001b[39;49mrequest\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[39m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[39m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[39m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection\u001b[39m.\u001b[39;49mhandle_request(request)\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mreceive_response_headers\u001b[39m\u001b[39m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_response_headers(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    107\u001b[0m     trace\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_event(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    178\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(event, h11\u001b[39m.\u001b[39mResponse):\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39mif\u001b[39;00m event \u001b[39mis\u001b[39;00m h11\u001b[39m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_network_stream\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    218\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mREAD_NUM_BYTES, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    219\u001b[0m     )\n\u001b[1;32m    221\u001b[0m     \u001b[39m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[39m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[39m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[39m# it as a ConnectError.\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    125\u001b[0m exc_map: ExceptionMapping \u001b[39m=\u001b[39m {socket\u001b[39m.\u001b[39mtimeout: ReadTimeout, \u001b[39mOSError\u001b[39;00m: ReadError}\n\u001b[0;32m--> 126\u001b[0m \u001b[39mwith\u001b[39;49;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49msettimeout(timeout)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.11-linux-x86_64-gnu/lib/python3.11/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen\u001b[39m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    159\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[39mraise\u001b[39;00m to_exc(exc) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mReadError\u001b[0m: [Errno 104] Connection reset by peer",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mReadError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/_base_client.py:1003\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1003\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msend(\n\u001b[1;32m   1004\u001b[0m         request,\n\u001b[1;32m   1005\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_should_stream_response_body(request\u001b[39m=\u001b[39;49mrequest),\n\u001b[1;32m   1006\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1007\u001b[0m     )\n\u001b[1;32m   1008\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_auth(\n\u001b[1;32m    915\u001b[0m     request,\n\u001b[1;32m    916\u001b[0m     auth\u001b[39m=\u001b[39;49mauth,\n\u001b[1;32m    917\u001b[0m     follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    918\u001b[0m     history\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m    919\u001b[0m )\n\u001b[1;32m    920\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_redirects(\n\u001b[1;32m    943\u001b[0m         request,\n\u001b[1;32m    944\u001b[0m         follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    945\u001b[0m         history\u001b[39m=\u001b[39;49mhistory,\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_single_request(request)\n\u001b[1;32m    980\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m   1016\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, SyncByteStream)\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:249\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[0;32m--> 249\u001b[0m \u001b[39mwith\u001b[39;49;00m map_httpcore_exceptions():\n\u001b[1;32m    250\u001b[0m     resp \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.11-linux-x86_64-gnu/lib/python3.11/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen\u001b[39m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    159\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:118\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m message \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(exc)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mraise\u001b[39;00m mapped_exc(message) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mexc\u001b[39;00m\n",
      "\u001b[0;31mReadError\u001b[0m: [Errno 104] Connection reset by peer",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:726\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 726\u001b[0m                 \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    727\u001b[0m \u001b[39mexcept\u001b[39;00m OpenAIError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:653\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    641\u001b[0m logging_obj\u001b[39m.\u001b[39mpre_call(\n\u001b[1;32m    642\u001b[0m     \u001b[39minput\u001b[39m\u001b[39m=\u001b[39mmessages,\n\u001b[1;32m    643\u001b[0m     api_key\u001b[39m=\u001b[39mopenai_client\u001b[39m.\u001b[39mapi_key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m     },\n\u001b[1;32m    650\u001b[0m )\n\u001b[1;32m    652\u001b[0m headers, response \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 653\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_sync_openai_chat_completion_request(\n\u001b[1;32m    654\u001b[0m         openai_client\u001b[39m=\u001b[39;49mopenai_client,\n\u001b[1;32m    655\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    656\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    657\u001b[0m         logging_obj\u001b[39m=\u001b[39;49mlogging_obj,\n\u001b[1;32m    658\u001b[0m     )\n\u001b[1;32m    659\u001b[0m )\n\u001b[1;32m    661\u001b[0m logging_obj\u001b[39m.\u001b[39mmodel_call_details[\u001b[39m\"\u001b[39m\u001b[39mresponse_headers\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m headers\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_utils.py:145\u001b[0m, in \u001b[0;36mtrack_llm_api_timing.<locals>.decorator.<locals>.sync_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    146\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:472\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[0;34m(self, openai_client, data, timeout, logging_obj)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 472\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:454\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[0;34m(self, openai_client, data, timeout, logging_obj)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     raw_response \u001b[39m=\u001b[39m openai_client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mwith_raw_response\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    455\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdata, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    456\u001b[0m     )\n\u001b[1;32m    458\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(raw_response, \u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mextra_headers\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[39mreturn\u001b[39;00m cast(LegacyAPIResponse[R], func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:879\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    878\u001b[0m validate_response_format(response_format)\n\u001b[0;32m--> 879\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    880\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    881\u001b[0m     body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    882\u001b[0m         {\n\u001b[1;32m    883\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    884\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    885\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m\"\u001b[39;49m: audio,\n\u001b[1;32m    886\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    887\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    888\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    889\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    890\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    891\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmax_completion_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_completion_tokens,\n\u001b[1;32m    892\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    893\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m: metadata,\n\u001b[1;32m    894\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmodalities\u001b[39;49m\u001b[39m\"\u001b[39;49m: modalities,\n\u001b[1;32m    895\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    896\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mparallel_tool_calls\u001b[39;49m\u001b[39m\"\u001b[39;49m: parallel_tool_calls,\n\u001b[1;32m    897\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mprediction\u001b[39;49m\u001b[39m\"\u001b[39;49m: prediction,\n\u001b[1;32m    898\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    899\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mreasoning_effort\u001b[39;49m\u001b[39m\"\u001b[39;49m: reasoning_effort,\n\u001b[1;32m    900\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    901\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    902\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mservice_tier\u001b[39;49m\u001b[39m\"\u001b[39;49m: service_tier,\n\u001b[1;32m    903\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    904\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstore\u001b[39;49m\u001b[39m\"\u001b[39;49m: store,\n\u001b[1;32m    905\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    906\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstream_options\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream_options,\n\u001b[1;32m    907\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    908\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    909\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    910\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[1;32m    911\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    912\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    913\u001b[0m         },\n\u001b[1;32m    914\u001b[0m         completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    915\u001b[0m     ),\n\u001b[1;32m    916\u001b[0m     options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    917\u001b[0m         extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    918\u001b[0m     ),\n\u001b[1;32m    919\u001b[0m     cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    920\u001b[0m     stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    921\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    922\u001b[0m )\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/_base_client.py:1290\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1287\u001b[0m opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1288\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1289\u001b[0m )\n\u001b[0;32m-> 1290\u001b[0m \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/_base_client.py:967\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    965\u001b[0m     retries_taken \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 967\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    968\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    969\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    970\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    971\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    972\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m    973\u001b[0m )\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/_base_client.py:1037\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1036\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRaising connection error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1037\u001b[0m     \u001b[39mraise\u001b[39;00m APIConnectionError(request\u001b[39m=\u001b[39mrequest) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39merr\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m log\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m   1040\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mHTTP Response: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1041\u001b[0m     request\u001b[39m.\u001b[39mmethod,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     response\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m   1046\u001b[0m )\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m: Connection error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/main.py:1723\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     logging\u001b[39m.\u001b[39mpost_call(\n\u001b[1;32m   1718\u001b[0m         \u001b[39minput\u001b[39m\u001b[39m=\u001b[39mmessages,\n\u001b[1;32m   1719\u001b[0m         api_key\u001b[39m=\u001b[39mapi_key,\n\u001b[1;32m   1720\u001b[0m         original_response\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(e),\n\u001b[1;32m   1721\u001b[0m         additional_args\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m: headers},\n\u001b[1;32m   1722\u001b[0m     )\n\u001b[0;32m-> 1723\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   1725\u001b[0m \u001b[39mif\u001b[39;00m optional_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1726\u001b[0m     \u001b[39m## LOGGING\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/main.py:1696\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1695\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1696\u001b[0m     response \u001b[39m=\u001b[39m openai_chat_completions\u001b[39m.\u001b[39;49mcompletion(\n\u001b[1;32m   1697\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1698\u001b[0m         messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m   1699\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1700\u001b[0m         model_response\u001b[39m=\u001b[39;49mmodel_response,\n\u001b[1;32m   1701\u001b[0m         print_verbose\u001b[39m=\u001b[39;49mprint_verbose,\n\u001b[1;32m   1702\u001b[0m         api_key\u001b[39m=\u001b[39;49mapi_key,\n\u001b[1;32m   1703\u001b[0m         api_base\u001b[39m=\u001b[39;49mapi_base,\n\u001b[1;32m   1704\u001b[0m         acompletion\u001b[39m=\u001b[39;49macompletion,\n\u001b[1;32m   1705\u001b[0m         logging_obj\u001b[39m=\u001b[39;49mlogging,\n\u001b[1;32m   1706\u001b[0m         optional_params\u001b[39m=\u001b[39;49moptional_params,\n\u001b[1;32m   1707\u001b[0m         litellm_params\u001b[39m=\u001b[39;49mlitellm_params,\n\u001b[1;32m   1708\u001b[0m         logger_fn\u001b[39m=\u001b[39;49mlogger_fn,\n\u001b[1;32m   1709\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   1710\u001b[0m         custom_prompt_dict\u001b[39m=\u001b[39;49mcustom_prompt_dict,\n\u001b[1;32m   1711\u001b[0m         client\u001b[39m=\u001b[39;49mclient,  \u001b[39m# pass AsyncOpenAI, OpenAI client\u001b[39;49;00m\n\u001b[1;32m   1712\u001b[0m         organization\u001b[39m=\u001b[39;49morganization,\n\u001b[1;32m   1713\u001b[0m         custom_llm_provider\u001b[39m=\u001b[39;49mcustom_llm_provider,\n\u001b[1;32m   1714\u001b[0m     )\n\u001b[1;32m   1715\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1716\u001b[0m     \u001b[39m## LOGGING - log the original exception returned\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:736\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    735\u001b[0m     error_headers \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(error_response, \u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 736\u001b[0m \u001b[39mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    737\u001b[0m     status_code\u001b[39m=\u001b[39mstatus_code, message\u001b[39m=\u001b[39merror_text, headers\u001b[39m=\u001b[39merror_headers\n\u001b[1;32m    738\u001b[0m )\n",
      "\u001b[0;31mOpenAIError\u001b[0m: Connection error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb Cell 17\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mAnkara is the capital of Turkey.\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mClaude Shannon is the father of information theory.\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m response \u001b[39m=\u001b[39m make_program()(text\u001b[39m=\u001b[39;49mtext)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(text)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/utils/callback.py:234\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39m# If no callbacks are provided, just call the function\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(instance, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    236\u001b[0m \u001b[39m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m call_id \u001b[39m=\u001b[39m uuid\u001b[39m.\u001b[39muuid4()\u001b[39m.\u001b[39mhex\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/predict/predict.py:67\u001b[0m, in \u001b[0;36mPredict.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m@with_callbacks\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/predict/predict.py:97\u001b[0m, in \u001b[0;36mPredict.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdspy\u001b[39;00m\n\u001b[1;32m     96\u001b[0m adapter \u001b[39m=\u001b[39m dspy\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39madapter \u001b[39mor\u001b[39;00m dspy\u001b[39m.\u001b[39mChatAdapter()\n\u001b[0;32m---> 97\u001b[0m completions \u001b[39m=\u001b[39m adapter(lm, lm_kwargs\u001b[39m=\u001b[39;49mconfig, signature\u001b[39m=\u001b[39;49msignature, demos\u001b[39m=\u001b[39;49mdemos, inputs\u001b[39m=\u001b[39;49mkwargs)\n\u001b[1;32m     99\u001b[0m pred \u001b[39m=\u001b[39m Prediction\u001b[39m.\u001b[39mfrom_completions(completions, signature\u001b[39m=\u001b[39msignature)\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_trace\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m) \u001b[39mand\u001b[39;00m dspy\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39mtrace \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/adapters/base.py:23\u001b[0m, in \u001b[0;36mAdapter.__call__\u001b[0;34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[0m\n\u001b[1;32m     20\u001b[0m inputs_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat(signature, demos, inputs)\n\u001b[1;32m     21\u001b[0m inputs_ \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(prompt\u001b[39m=\u001b[39minputs_) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs_, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mdict\u001b[39m(messages\u001b[39m=\u001b[39minputs_)\n\u001b[0;32m---> 23\u001b[0m outputs \u001b[39m=\u001b[39m lm(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs_, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mlm_kwargs)\n\u001b[1;32m     24\u001b[0m values \u001b[39m=\u001b[39m []\n\u001b[1;32m     26\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/utils/callback.py:234\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39m# If no callbacks are provided, just call the function\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(instance, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    236\u001b[0m \u001b[39m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m call_id \u001b[39m=\u001b[39m uuid\u001b[39m.\u001b[39muuid4()\u001b[39m.\u001b[39mhex\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/clients/lm.py:119\u001b[0m, in \u001b[0;36mLM.__call__\u001b[0;34m(self, prompt, messages, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     completion \u001b[39m=\u001b[39m litellm_completion \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mchat\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m litellm_text_completion\n\u001b[0;32m--> 119\u001b[0m     response \u001b[39m=\u001b[39m completion(\n\u001b[1;32m    120\u001b[0m         request\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, messages\u001b[39m=\u001b[39;49mmessages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs),\n\u001b[1;32m    121\u001b[0m         num_retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_retries,\n\u001b[1;32m    122\u001b[0m         \u001b[39m# only leverage LiteLLM cache in this case\u001b[39;49;00m\n\u001b[1;32m    123\u001b[0m         cache\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mno-cache\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mnot\u001b[39;49;00m cache, \u001b[39m\"\u001b[39;49m\u001b[39mno-store\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mnot\u001b[39;49;00m cache},\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    127\u001b[0m     outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    128\u001b[0m         {\n\u001b[1;32m    129\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: c\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(c, \u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m c[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    133\u001b[0m     ]\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/clients/lm.py:350\u001b[0m, in \u001b[0;36mlitellm_completion\u001b[0;34m(request, num_retries, cache)\u001b[0m\n\u001b[1;32m    348\u001b[0m stream \u001b[39m=\u001b[39m dspy\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39msend_stream\n\u001b[1;32m    349\u001b[0m \u001b[39mif\u001b[39;00m stream \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 350\u001b[0m     \u001b[39mreturn\u001b[39;00m litellm\u001b[39m.\u001b[39;49mcompletion(\n\u001b[1;32m    351\u001b[0m         cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m    352\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mretry_kwargs,\n\u001b[1;32m    353\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest,\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    356\u001b[0m \u001b[39m# The stream is already opened, and will be closed by the caller.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m stream \u001b[39m=\u001b[39m cast(MemoryObjectSendStream, stream)\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/utils.py:1190\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39mif\u001b[39;00m logging_obj:\n\u001b[1;32m   1187\u001b[0m     logging_obj\u001b[39m.\u001b[39mfailure_handler(\n\u001b[1;32m   1188\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1189\u001b[0m     )  \u001b[39m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/utils.py:1068\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         print_verbose(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError while checking max token limit: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1067\u001b[0m \u001b[39m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1068\u001b[0m result \u001b[39m=\u001b[39m original_function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1069\u001b[0m end_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m   1070\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m kwargs[\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/main.py:3084\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   3081\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[1;32m   3082\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   3083\u001b[0m     \u001b[39m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 3084\u001b[0m     \u001b[39mraise\u001b[39;00m exception_type(\n\u001b[1;32m   3085\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   3086\u001b[0m         custom_llm_provider\u001b[39m=\u001b[39;49mcustom_llm_provider,\n\u001b[1;32m   3087\u001b[0m         original_exception\u001b[39m=\u001b[39;49me,\n\u001b[1;32m   3088\u001b[0m         completion_kwargs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   3089\u001b[0m         extra_kwargs\u001b[39m=\u001b[39;49mkwargs,\n\u001b[1;32m   3090\u001b[0m     )\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2202\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2200\u001b[0m \u001b[39mif\u001b[39;00m exception_mapping_worked:\n\u001b[1;32m   2201\u001b[0m     \u001b[39msetattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mlitellm_response_headers\u001b[39m\u001b[39m\"\u001b[39m, litellm_response_headers)\n\u001b[0;32m-> 2202\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   2203\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2204\u001b[0m     \u001b[39mfor\u001b[39;00m error_type \u001b[39min\u001b[39;00m litellm\u001b[39m.\u001b[39mLITELLM_EXCEPTION_TYPES:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:452\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m         exception_mapping_worked \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 452\u001b[0m         \u001b[39mraise\u001b[39;00m APIError(\n\u001b[1;32m    453\u001b[0m             status_code\u001b[39m=\u001b[39moriginal_exception\u001b[39m.\u001b[39mstatus_code,\n\u001b[1;32m    454\u001b[0m             message\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAPIError: \u001b[39m\u001b[39m{\u001b[39;00mexception_provider\u001b[39m}\u001b[39;00m\u001b[39m - \u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    455\u001b[0m             llm_provider\u001b[39m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m    456\u001b[0m             model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    457\u001b[0m             request\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(original_exception, \u001b[39m\"\u001b[39m\u001b[39mrequest\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    458\u001b[0m             litellm_debug_info\u001b[39m=\u001b[39mextra_information,\n\u001b[1;32m    459\u001b[0m         )\n\u001b[1;32m    460\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39m# if no status code then it is an APIConnectionError: https://github.com/openai/openai-python#handling-errors\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[39m# exception_mapping_worked = True\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[39mraise\u001b[39;00m APIConnectionError(\n\u001b[1;32m    464\u001b[0m         message\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAPIConnectionError: \u001b[39m\u001b[39m{\u001b[39;00mexception_provider\u001b[39m}\u001b[39;00m\u001b[39m - \u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    465\u001b[0m         llm_provider\u001b[39m=\u001b[39mcustom_llm_provider,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    470\u001b[0m         ),\n\u001b[1;32m    471\u001b[0m     )\n",
      "\u001b[0;31mAPIError\u001b[0m: litellm.APIError: APIError: OpenAIException - Connection error."
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Ankara is the capital of Turkey.\n",
    "Claude Shannon is the father of information theory.\n",
    "\"\"\".strip()\n",
    "\n",
    "response = make_program()(text=text)\n",
    "print(text)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset_path: str,\n",
    "    dataset_name: str,\n",
    "    dataset_split: str,\n",
    "    model: str,\n",
    "    temperature: float,\n",
    "    optimizer_config: dict,\n",
    "    out: str,\n",
    "):\n",
    "    # Set up LM\n",
    "    configure_lm(model, temperature)\n",
    "\n",
    "    # Load dataset\n",
    "    ds = load_dataset(dataset_path, dataset_name, split=dataset_split)\n",
    "    examples = [dspy.Example(text=x[\"text\"], triples=x[\"triples\"]).with_inputs(\"text\") for x in ds]\n",
    "\n",
    "    # Create program\n",
    "    program = make_program()\n",
    "\n",
    "    # Create and run optimizer\n",
    "    optimizer = make_optimizer(optimizer_config)\n",
    "    compile_params = optimizer_config.get(\"compile_params\", {})\n",
    "    trained_program = optimizer.compile(program, trainset=examples, **compile_params)\n",
    "\n",
    "    # Save trained program\n",
    "    Path(out).parent.mkdir(parents=True, exist_ok=True)\n",
    "    trained_program.save(out)\n",
    "    return trained_program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    dataset_path: str,\n",
    "    dataset_name: str,\n",
    "    dataset_split: str,\n",
    "    model: str,\n",
    "    temperature: float,\n",
    "    out_dir: str,\n",
    "    load_from: str | None = None,\n",
    "):\n",
    "    # Set up LM\n",
    "    configure_lm(model, temperature)\n",
    "\n",
    "    # Load dataset\n",
    "    ds = load_dataset(dataset_path, dataset_name, split=dataset_split)\n",
    "    examples = [dspy.Example(text=x[\"text\"], triples=x[\"triples\"]).with_inputs(\"text\") for x in ds]\n",
    "\n",
    "    # Load program\n",
    "    program = make_program()\n",
    "    if load_from:\n",
    "        program.load(load_from)\n",
    "\n",
    "    # Evaluate\n",
    "    evaluator = Evaluate(\n",
    "        metric=evaluate_triples,\n",
    "        devset=examples,\n",
    "        num_threads=16,\n",
    "        display_progress=True,\n",
    "        return_outputs=True,\n",
    "    )\n",
    "    _, results = evaluator(program)\n",
    "\n",
    "    # Save results\n",
    "    out_path = Path(out_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Process and save detailed results\n",
    "    processed_results = []\n",
    "    for example, pred, score in results:\n",
    "        result = {\n",
    "            \"text\": example.text,\n",
    "            \"triples\": example.triples,\n",
    "            \"predicted_triples\": parse_triples(pred.triples_str),\n",
    "            **compute_scores(parse_triples(pred.triples_str), example.triples),\n",
    "        }\n",
    "        processed_results.append(result)\n",
    "    \n",
    "    result_df = pd.DataFrame(processed_results)\n",
    "    result_df.to_json(out_path / \"results.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "    # Save aggregate scores\n",
    "    scores = result_df[[\"exact.precision\", \"exact.recall\", \"exact.f1\", \"fuzzy.precision\", \"fuzzy.recall\", \"fuzzy.f1\"]].mean().to_dict()\n",
    "    with open(out_path / \"scores.json\", \"w\") as f:\n",
    "        json.dump(scores, f, indent=2)\n",
    "\n",
    "    return results, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"bdsaglam/web_nlg-erx-concat\"\n",
    "dataset_name = \"release_v3.0_en\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before prompt optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "results, scores = evaluate(\n",
    "    dataset_path=dataset_path,\n",
    "    dataset_name=dataset_name,\n",
    "    dataset_split=\"dev[:100]\",\n",
    "    model=\"llama-3-8b\",\n",
    "    temperature=0.0,\n",
    "    out_dir=\"../tmp/erx/dspy/before\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact.precision': 0.012619047619047618,\n",
       " 'exact.recall': 0.01310883968972204,\n",
       " 'exact.f1': 0.012762189200004326,\n",
       " 'fuzzy.precision': 0.36535511660831355,\n",
       " 'fuzzy.recall': 0.3532727740199849,\n",
       " 'fuzzy.f1': 0.3564569059040341}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's optimize the program with DSPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 8 traces per predictor.\n",
      "Will attempt to bootstrap 16 candidate sets.\n",
      "Average Metric: 32.95 / 100 (33.0%): 100%|| 100/100 [00:16<00:00,  5.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 15:46:28 INFO dspy.evaluate.evaluate: Average Metric: 32.95016723063732 / 100 (33.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 32.95 for seed -3\n",
      "Scores so far: [32.95]\n",
      "Best score so far: 32.95\n",
      "Average Metric: 33.69 / 100 (33.7%): 100%|| 100/100 [00:17<00:00,  5.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 15:46:46 INFO dspy.evaluate.evaluate: Average Metric: 33.69250472074029 / 100 (33.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 33.69 for seed -2\n",
      "Scores so far: [32.95, 33.69]\n",
      "Best score so far: 33.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 8/100 [00:19<03:40,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 16 rounds, amounting to 14 attempts.\n",
      "Average Metric: 36.90 / 100 (36.9%): 100%|| 100/100 [00:22<00:00,  4.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 15:47:28 INFO dspy.evaluate.evaluate: Average Metric: 36.904625085136594 / 100 (36.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 36.9 for seed -1\n",
      "Scores so far: [32.95, 33.69, 36.9]\n",
      "Best score so far: 36.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 7/100 [00:16<03:37,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 7 full traces after 7 examples for up to 16 rounds, amounting to 18 attempts.\n",
      "Average Metric: 0.00 / 4 (0.0%):   4%|         | 4/100 [00:01<00:32,  2.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 15:47:49 ERROR dspy.utils.parallelizer: Error processing item Example({'text': 'Alan Bean was a crew member of Apollo 12.\\nBorn in Imst (in Austria-Hungary), Alfons Gorbach died in Graz, in Styria.\\nAleksandre Guruli played for the Olympique Lyonnais club who play their home games at the Parc Olympique Lyonnais.\\nTrane is a manufacturer of building materials, including building management systems and HVAC.\\nChristopher Taylor, politician, leads Ann Arbor, Michigan.\\nThe ISSN number for Abhandlungen aus dem Mathematischen Seminar der Universitat Hamburg (abbreviating to Abh. Math. Semin. Univ. Hambg.) is 1865-8784. The establishment is concerned with the academic discipline of Pure Mathematics.\\nVfl Wolfsburg play in the Bundesliga.', 'triples': ['Alan Bean | mission | Apollo 12', 'Alfons Gorbach | death place | Styria', 'Alfons Gorbach | death place | Graz', 'Alfons Gorbach | birth place | Austria-Hungary', 'Alfons Gorbach | birth place | Imst', 'Aleksandre Guruli | club | Olympique Lyonnais', 'Olympique Lyonnais | ground | Parc Olympique Lyonnais', 'Trane | product | Building Management System', 'Trane | industry | Building materials', 'Trane | product | HVAC', 'Ann Arbor, Michigan | leader | Christopher Taylor (politician)', 'Abhandlungen aus dem Mathematischen Seminar der Universitt Hamburg | issn number | 1865-8784', 'Abhandlungen aus dem Mathematischen Seminar der Universitt Hamburg | academic discipline | Pure mathematics', 'Abhandlungen aus dem Mathematischen Seminar der Universitt Hamburg | abbreviation | Abh. Math. Semin. Univ. Hambg.', 'VfL Wolfsburg | league | Bundesliga']}) (input_keys={'text'}): litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': {'message': 'litellm.BadRequestError: OpenAIException - Failed to deserialize the JSON body into the target type: response_format: missing field `value` at line 1 column 9843\\nReceived Model Group=llama-3-8b\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '400'}}. Set `provide_traceback=True` to see the stack trace.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 4 (0.0%):   5%|         | 5/100 [00:04<02:12,  1.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 15:47:49 ERROR dspy.utils.parallelizer: Error processing item Example({'text': \"Alfred N. Phillips was a member of the US Army, which fought in the Whiskey Rebellion.\\nAndrew Rayel is associated with a number of musical artists, these include: Armin Van Buuren, Bobina, Mark Sixma, Jonathan Mendelsohn, Christian Burns, Jwaydan, Alexander Popov, Jano, Alexandre Bergheau, Jonny Rose, Sylvia Tosun, Lira Yin, and Alexandra Badoi.\\n1634 The Ram Rebellion comes from the United States where the leader is known as the President and the Native Americans are an ethnic group.\\nAlison O'Donnell plays the instrument called the bodhran.\\nAMC Matador, known also as the American Motors Matador, has an AMC V8 engine and was made in Australia.\\nThe musical genre of american, Ahmet Ertegun, is rhythm and blues, a derivative of which is disco.\", 'triples': ['Alfred N. Phillips | military branch | United States Army', 'United States Army | battle | Whiskey Rebellion', 'Andrew Rayel | associated band/associated musical artist | Armin Van Buuren, Bobina, Mark Sixma, Jonathan Mendelsohn, Christian Burns, Jwaydan, Alexander Popov, Jano, Alexandre Bergheau, Jonny Rose, Sylvia Tosun, Lira Yin, Alexandra Badoi', 'United States | leader title | President of the United States', '1634: The Ram Rebellion | country | United States', 'United States | ethnic group | Native Americans in the United States', \"Alison O'Donnell | instrument | Bodhrn\", 'AMC Matador | alternative name | American Motors Matador', 'AMC Matador | assembly | Australia', 'AMC Matador | engine | AMC V8 engine', 'Ahmet Ertegun | genre | Rhythm and blues', 'Rhythm and blues | derivative | Disco', 'Ahmet Ertegun | origin | United States']}) (input_keys={'text'}): litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': {'message': 'litellm.BadRequestError: OpenAIException - Failed to deserialize the JSON body into the target type: response_format: missing field `value` at line 1 column 9936\\nReceived Model Group=llama-3-8b\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '400'}}. Set `provide_traceback=True` to see the stack trace.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 4 (0.0%):   6%|         | 6/100 [00:04<01:30,  1.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 15:47:50 ERROR dspy.utils.parallelizer: Error processing item Example({'text': 'Doris Bures is the leader of Austria where Alfons Gorbach died in Styria.\\n10 Hygiea has an escape velocity of 0.21 kilometres per second and an apoapsis of 523951582.33968 kilometres.\\nThe 1st runway at Alderney Airport is made from Poaceae which is member of the Poales order. Poaceae belongs to the Commelinids order, within the flowering plants and classed as Monocotyledon.\\n\"The Secret Scripture\" followed the book \"A Long Long Way,\" which was preceded by \"Annie Dunne\".', 'triples': ['Austria | leader | Doris Bures', 'Alfons Gorbach | death place | Styria', 'Alfons Gorbach | death place | Austria', '10 Hygiea | escape velocity | 0.21 (kilometrePerSeconds)', '10 Hygiea | apoapsis | 523951582.33968 (kilometres)', 'Poaceae | division | Flowering plant', 'Alderney Airport | 1st runway surface type | Poaceae', 'Poaceae | order | Poales', 'Poaceae | order | Commelinids', 'Poaceae | class | Monocotyledon', 'A Long Long Way | preceded by | Annie Dunne', 'A Long Long Way | followed by | The Secret Scripture']}) (input_keys={'text'}): litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': {'message': 'litellm.BadRequestError: OpenAIException - Failed to deserialize the JSON body into the target type: response_format: missing field `value` at line 1 column 9664\\nReceived Model Group=llama-3-8b\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '400'}}. Set `provide_traceback=True` to see the stack trace.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 4 (0.0%):   6%|         | 6/100 [00:05<01:30,  1.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 15:47:50 ERROR dspy.utils.parallelizer: Error processing item Example({'text': \"AS Roma's manager is Luciano Spalletti, who played for Udinese Calcio. He plays for both, Empoli F.C. and Virtus Entella.\\nThe Atlas II is from the United States, where the capital is Washington D.C. and the language English. The leader of the United States is the President of the United States and one of its ethnic groups is Asian Americans.\\nThe runway length of Afonso Pena International Airport is 2215.0.\\nAWH Engineering College at Kerala has 250 academic staff. Kerala's leader is Kochi.\\nAbove the Veil followed the book Aenir.\\nThe ISBN number of Aenir is 0-439-17684-0.\\nTarrant County, with the largest city of Fort Worth, is home to Arlington, Texas, United States.\", 'triples': ['Luciano Spalletti | club | Udinese Calcio', 'Luciano Spalletti | club | Empoli F.C.', 'A.S. Roma | manager | Luciano Spalletti', 'Luciano Spalletti | club | Virtus Entella', 'United States | ethnic group | Asian Americans', 'United States | capital | Washington, D.C.', 'United States | language | English language', 'Atlas II | country origin | United States', 'United States | leader title | President of the United States', 'Afonso Pena International Airport | runway length | 2215.0', 'Kerala | leader | Kochi', 'AWH Engineering College | academic staff size | 250', 'AWH Engineering College | state | Kerala', 'Aenir | followed by | Above the Veil', 'Aenir | isbn number | 0-439-17684-0', 'Tarrant County, Texas | largest city | Fort Worth, Texas', 'Arlington, Texas | is part of | Tarrant County, Texas', 'Arlington, Texas | is part of | Texas', 'Texas | country | United States']}) (input_keys={'text'}): litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': {'message': 'litellm.BadRequestError: OpenAIException - Failed to deserialize the JSON body into the target type: response_format: missing field `value` at line 1 column 9861\\nReceived Model Group=llama-3-8b\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '400'}}. Set `provide_traceback=True` to see the stack trace.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 4 (0.0%):   7%|         | 7/100 [00:05<01:09,  1.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 15:47:50 ERROR dspy.utils.parallelizer: Error processing item Example({'text': 'Akeem Priestley is connected to the Orange County Blues Football club which is managed by Oliver Wyss, but he also plays for Jacksonville Dolphins who have their baseball field in the John Sessions stadium.\\nWilliam Anders (born in British Hong Kong) was a crew member of Apollo 8 and was selected by NASA in 1963. He retired on 1969-09-01.', 'triples': ['Orange County Blues FC | manager | Oliver Wyss', 'Akeem Priestley | club | Orange County Blues FC', 'Akeem Priestley | club | Jacksonville Dolphins', 'Jacksonville Dolphins | stadium | John Sessions Stadium', 'William Anders | date of retirement | 1969-09-01', 'William Anders | selected by nasa | 1963', 'William Anders | birth place | British Hong Kong', 'William Anders | mission | Apollo 8']}) (input_keys={'text'}): litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': {'message': 'litellm.BadRequestError: OpenAIException - Failed to deserialize the JSON body into the target type: response_format: missing field `value` at line 1 column 9522\\nReceived Model Group=llama-3-8b\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '400'}}. Set `provide_traceback=True` to see the stack trace.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 4 (0.0%):   9%|         | 9/100 [00:05<00:42,  2.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 15:47:50 ERROR dspy.utils.parallelizer: Error processing item Example({'text': 'Alpena County Regional Airport is located in Wilson Township, Alpena County, Michigan, United States. The airport has a runway length of 1533.0 and is at an elevation of 210 metres above sea level.\\nThe temperature of 1097 Vicia is 171.0 (kelvins) and its apoapsis is 511592000.0 km.\\nAdam Holloway was born in Kent and began his career on May 5th 2005. He is a membe of the Conservative Party in the UK and served in the Grenadier Guards. His alma mater is Magdalene College, Cambridge.', 'triples': ['Alpena County Regional Airport | location | Wilson Township, Alpena County, Michigan', 'Wilson Township, Alpena County, Michigan | country | United States', 'Alpena County Regional Airport | elevation above the sea level | 210', 'Alpena County Regional Airport | runway length | 1533.0', '1097 Vicia | apoapsis | 511592000.0 (kilometres)', '1097 Vicia | temperature | 171.0 (kelvins)', 'Adam Holloway | party | Conservative Party (UK)', 'Adam Holloway | birth place | Kent', 'Adam Holloway | active years start date | 2005-05-05', 'Adam Holloway | military branch | Grenadier Guards', 'Adam Holloway | alma mater | Magdalene College, Cambridge']}) (input_keys={'text'}): litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': {'message': 'litellm.BadRequestError: OpenAIException - Failed to deserialize the JSON body into the target type: response_format: missing field `value` at line 1 column 9669\\nReceived Model Group=llama-3-8b\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '400'}}. Set `provide_traceback=True` to see the stack trace.\n",
      "2025/02/14 15:47:50 ERROR dspy.utils.parallelizer: Error processing item Example({'text': 'Aenir written by Garth Nix was produced in print has an OCLC number of 45644811 and an ISBN number of 0-439-17684-0.\\nThe ALCO RS-3 was built by the Montreal Locomotive Works between May 1950 and August 1956. It has a V12 engine and a cylinder count of 12.\\nThe Ataturk Monument (Izmir) is located in Turkey and was inaugurated on 1932-07-27. It was designed by Pietro Canonica and is made of bronze.\\nAbraham A. Ribicoff, who was married to Casey Ribicoff, was born in New Britain, Connecticut.\\nChristmas pudding is an ingredient in Baked Alaska, that is from France and New York region.\\nAmatriciana sauce is from the Lazio region.\\nAlberto Teisaire belonged to the Justicialist Party and was in office while Juan Pern, the husband of Eva Peron, was president. Teisaire died in Buenos Aires where Diego Santilli is leader.', 'triples': ['Aenir | oclc number | 45644811', 'Aenir | author | Garth Nix', 'Aenir | media type | Print', 'Aenir | isbn number | 0-439-17684-0', 'ALCO RS-3 | builder | Montreal Locomotive Works', 'ALCO RS-3 | cylinder count | 12', 'ALCO RS-3 | engine | V12 engine', 'ALCO RS-3 | build date | May 1950 - August 1956', 'Atatrk Monument (zmir) | inauguration date | 1932-07-27', 'Atatrk Monument (zmir) | location | Turkey', 'Atatrk Monument (zmir) | material | Bronze', 'Atatrk Monument (zmir) | designer | Pietro Canonica', 'Abraham A. Ribicoff | spouse | Casey Ribicoff', 'Abraham A. Ribicoff | birth place | New Britain, Connecticut', 'Baked Alaska | country | France', 'Baked Alaska | region | New York', 'Baked Alaska | ingredient | Christmas pudding', 'Amatriciana sauce | region | Lazio', 'Alberto Teisaire | death place | Buenos Aires', 'Buenos Aires | leader | Diego Santilli', 'Juan Pern | spouse | Eva Pern', 'Alberto Teisaire | party | Justicialist Party', 'Alberto Teisaire | in office while president | Juan Pern']}) (input_keys={'text'}): litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': {'message': 'litellm.BadRequestError: OpenAIException - Failed to deserialize the JSON body into the target type: response_format: missing field `value` at line 1 column 10013\\nReceived Model Group=llama-3-8b\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '400'}}. Set `provide_traceback=True` to see the stack trace.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 4 (0.0%):   9%|         | 9/100 [00:05<00:42,  2.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 15:47:50 ERROR dspy.utils.parallelizer: Error processing item Example({'text': \"Huseyin Butuner and Hilmi Guner designed the red granite and white marble Baku Turkish Martyrs memorial which is dedicated to the Ottoman army soldiers killed in the Battle of Baku. The memorial is located in Azerbaijan where the leader is Artur Rasizade.\\nThe leader of the United States has the title President of the United States.\\nCleveland's governing body is Cleveland City Council.\", 'triples': [\"Baku Turkish Martyrs' Memorial | material | Red granite and white marble\", \"Baku Turkish Martyrs' Memorial | dedicated to | Ottoman Army soldiers killed in the Battle of Baku\", \"Baku Turkish Martyrs' Memorial | location | Azerbaijan\", 'Azerbaijan | leader | Artur Rasizade', \"Baku Turkish Martyrs' Memorial | designer | Hseyin Btner and Hilmi Gner\", 'United States | leader title | President of the United States', 'Cleveland | governing body | Cleveland City Council']}) (input_keys={'text'}): litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': {'message': 'litellm.BadRequestError: OpenAIException - Failed to deserialize the JSON body into the target type: response_format: missing field `value` at line 1 column 9571\\nReceived Model Group=llama-3-8b\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '400'}}. Set `provide_traceback=True` to see the stack trace.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 4 (0.0%):  11%|         | 11/100 [00:05<00:27,  3.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 15:47:50 ERROR dspy.utils.parallelizer: Error processing item Example({'text': 'African Americans are one of the ethnic groups in the United States, which is led by the President. Also in the U.S., is Angola which is part of the state of Indiana.\\nAlpena, Michigan is located in the United States.\\nAaron Hunt plays for SV Werder Bremen which is managed by Viktor Skrypnyk.', 'triples': ['United States | ethnic group | African Americans', 'Angola, Indiana | is part of | Indiana', 'United States | leader title | President of the United States', 'Angola, Indiana | country | United States', 'Alpena, Michigan | country | United States', 'Aaron Hunt | club | SV Werder Bremen', 'SV Werder Bremen | manager | Viktor Skrypnyk']}) (input_keys={'text'}): litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': {'message': 'litellm.BadRequestError: OpenAIException - Failed to deserialize the JSON body into the target type: response_format: missing field `value` at line 1 column 9475\\nReceived Model Group=llama-3-8b\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '400'}}. Set `provide_traceback=True` to see the stack trace.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 4 (0.0%):  12%|        | 12/100 [00:05<00:23,  3.76it/s]"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': {'message': 'litellm.BadRequestError: OpenAIException - Failed to deserialize the JSON body into the target type: response_format: missing field `value` at line 1 column 9585\\nReceived Model Group=llama-3-8b\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '400'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:726\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 726\u001b[0m                 \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    727\u001b[0m \u001b[39mexcept\u001b[39;00m OpenAIError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:653\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    641\u001b[0m logging_obj\u001b[39m.\u001b[39mpre_call(\n\u001b[1;32m    642\u001b[0m     \u001b[39minput\u001b[39m\u001b[39m=\u001b[39mmessages,\n\u001b[1;32m    643\u001b[0m     api_key\u001b[39m=\u001b[39mopenai_client\u001b[39m.\u001b[39mapi_key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m     },\n\u001b[1;32m    650\u001b[0m )\n\u001b[1;32m    652\u001b[0m headers, response \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 653\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_sync_openai_chat_completion_request(\n\u001b[1;32m    654\u001b[0m         openai_client\u001b[39m=\u001b[39;49mopenai_client,\n\u001b[1;32m    655\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    656\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    657\u001b[0m         logging_obj\u001b[39m=\u001b[39;49mlogging_obj,\n\u001b[1;32m    658\u001b[0m     )\n\u001b[1;32m    659\u001b[0m )\n\u001b[1;32m    661\u001b[0m logging_obj\u001b[39m.\u001b[39mmodel_call_details[\u001b[39m\"\u001b[39m\u001b[39mresponse_headers\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m headers\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_utils.py:145\u001b[0m, in \u001b[0;36mtrack_llm_api_timing.<locals>.decorator.<locals>.sync_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    146\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:472\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[0;34m(self, openai_client, data, timeout, logging_obj)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 472\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:454\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[0;34m(self, openai_client, data, timeout, logging_obj)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     raw_response \u001b[39m=\u001b[39m openai_client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mwith_raw_response\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    455\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdata, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    456\u001b[0m     )\n\u001b[1;32m    458\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(raw_response, \u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mextra_headers\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[39mreturn\u001b[39;00m cast(LegacyAPIResponse[R], func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:879\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    878\u001b[0m validate_response_format(response_format)\n\u001b[0;32m--> 879\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    880\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    881\u001b[0m     body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    882\u001b[0m         {\n\u001b[1;32m    883\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    884\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    885\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m\"\u001b[39;49m: audio,\n\u001b[1;32m    886\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    887\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    888\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    889\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    890\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    891\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmax_completion_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_completion_tokens,\n\u001b[1;32m    892\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    893\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m: metadata,\n\u001b[1;32m    894\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmodalities\u001b[39;49m\u001b[39m\"\u001b[39;49m: modalities,\n\u001b[1;32m    895\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    896\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mparallel_tool_calls\u001b[39;49m\u001b[39m\"\u001b[39;49m: parallel_tool_calls,\n\u001b[1;32m    897\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mprediction\u001b[39;49m\u001b[39m\"\u001b[39;49m: prediction,\n\u001b[1;32m    898\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    899\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mreasoning_effort\u001b[39;49m\u001b[39m\"\u001b[39;49m: reasoning_effort,\n\u001b[1;32m    900\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    901\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    902\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mservice_tier\u001b[39;49m\u001b[39m\"\u001b[39;49m: service_tier,\n\u001b[1;32m    903\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    904\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstore\u001b[39;49m\u001b[39m\"\u001b[39;49m: store,\n\u001b[1;32m    905\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    906\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstream_options\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream_options,\n\u001b[1;32m    907\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    908\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    909\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    910\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[1;32m    911\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    912\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    913\u001b[0m         },\n\u001b[1;32m    914\u001b[0m         completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    915\u001b[0m     ),\n\u001b[1;32m    916\u001b[0m     options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    917\u001b[0m         extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    918\u001b[0m     ),\n\u001b[1;32m    919\u001b[0m     cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    920\u001b[0m     stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    921\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    922\u001b[0m )\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/_base_client.py:1290\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1287\u001b[0m opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1288\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1289\u001b[0m )\n\u001b[0;32m-> 1290\u001b[0m \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/_base_client.py:967\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    965\u001b[0m     retries_taken \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 967\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    968\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    969\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    970\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    971\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    972\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m    973\u001b[0m )\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/_base_client.py:1071\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1071\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[1;32m   1074\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1075\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1079\u001b[0m     retries_taken\u001b[39m=\u001b[39mretries_taken,\n\u001b[1;32m   1080\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'litellm.BadRequestError: OpenAIException - Failed to deserialize the JSON body into the target type: response_format: missing field `value` at line 1 column 9585\\nReceived Model Group=llama-3-8b\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '400'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/main.py:1723\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     logging\u001b[39m.\u001b[39mpost_call(\n\u001b[1;32m   1718\u001b[0m         \u001b[39minput\u001b[39m\u001b[39m=\u001b[39mmessages,\n\u001b[1;32m   1719\u001b[0m         api_key\u001b[39m=\u001b[39mapi_key,\n\u001b[1;32m   1720\u001b[0m         original_response\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(e),\n\u001b[1;32m   1721\u001b[0m         additional_args\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m: headers},\n\u001b[1;32m   1722\u001b[0m     )\n\u001b[0;32m-> 1723\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   1725\u001b[0m \u001b[39mif\u001b[39;00m optional_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1726\u001b[0m     \u001b[39m## LOGGING\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/main.py:1696\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1695\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1696\u001b[0m     response \u001b[39m=\u001b[39m openai_chat_completions\u001b[39m.\u001b[39;49mcompletion(\n\u001b[1;32m   1697\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1698\u001b[0m         messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m   1699\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1700\u001b[0m         model_response\u001b[39m=\u001b[39;49mmodel_response,\n\u001b[1;32m   1701\u001b[0m         print_verbose\u001b[39m=\u001b[39;49mprint_verbose,\n\u001b[1;32m   1702\u001b[0m         api_key\u001b[39m=\u001b[39;49mapi_key,\n\u001b[1;32m   1703\u001b[0m         api_base\u001b[39m=\u001b[39;49mapi_base,\n\u001b[1;32m   1704\u001b[0m         acompletion\u001b[39m=\u001b[39;49macompletion,\n\u001b[1;32m   1705\u001b[0m         logging_obj\u001b[39m=\u001b[39;49mlogging,\n\u001b[1;32m   1706\u001b[0m         optional_params\u001b[39m=\u001b[39;49moptional_params,\n\u001b[1;32m   1707\u001b[0m         litellm_params\u001b[39m=\u001b[39;49mlitellm_params,\n\u001b[1;32m   1708\u001b[0m         logger_fn\u001b[39m=\u001b[39;49mlogger_fn,\n\u001b[1;32m   1709\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   1710\u001b[0m         custom_prompt_dict\u001b[39m=\u001b[39;49mcustom_prompt_dict,\n\u001b[1;32m   1711\u001b[0m         client\u001b[39m=\u001b[39;49mclient,  \u001b[39m# pass AsyncOpenAI, OpenAI client\u001b[39;49;00m\n\u001b[1;32m   1712\u001b[0m         organization\u001b[39m=\u001b[39;49morganization,\n\u001b[1;32m   1713\u001b[0m         custom_llm_provider\u001b[39m=\u001b[39;49mcustom_llm_provider,\n\u001b[1;32m   1714\u001b[0m     )\n\u001b[1;32m   1715\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1716\u001b[0m     \u001b[39m## LOGGING - log the original exception returned\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:736\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    735\u001b[0m     error_headers \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(error_response, \u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 736\u001b[0m \u001b[39mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    737\u001b[0m     status_code\u001b[39m=\u001b[39mstatus_code, message\u001b[39m=\u001b[39merror_text, headers\u001b[39m=\u001b[39merror_headers\n\u001b[1;32m    738\u001b[0m )\n",
      "\u001b[0;31mOpenAIError\u001b[0m: Error code: 400 - {'error': {'message': 'litellm.BadRequestError: OpenAIException - Failed to deserialize the JSON body into the target type: response_format: missing field `value` at line 1 column 9585\\nReceived Model Group=llama-3-8b\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '400'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m optimizer_config \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mBootstrapFewShotWithRandomSearch\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwith_metric\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m }\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m train(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     dataset_path\u001b[39m=\u001b[39;49mdataset_path,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     dataset_name\u001b[39m=\u001b[39;49mdataset_name,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     dataset_split\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain[:100]\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mllama-3-8b\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     optimizer_config\u001b[39m=\u001b[39;49moptimizer_config,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     out\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../tmp/erx/dspy/trained-program.json\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m )\n",
      "\u001b[1;32m/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer \u001b[39m=\u001b[39m make_optimizer(optimizer_config)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m compile_params \u001b[39m=\u001b[39m optimizer_config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcompile_params\u001b[39m\u001b[39m\"\u001b[39m, {})\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m trained_program \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39;49mcompile(program, trainset\u001b[39m=\u001b[39;49mexamples, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcompile_params)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Save trained program\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/prompt_optimization.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m Path(out)\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mmkdir(parents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/teleprompt/random_search.py:119\u001b[0m, in \u001b[0;36mBootstrapFewShotWithRandomSearch.compile\u001b[0;34m(self, student, teacher, trainset, valset, restrict, labeled_sample)\u001b[0m\n\u001b[1;32m    108\u001b[0m     program \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mcompile(student, teacher\u001b[39m=\u001b[39mteacher, trainset\u001b[39m=\u001b[39mtrainset_copy)\n\u001b[1;32m    110\u001b[0m evaluate \u001b[39m=\u001b[39m Evaluate(\n\u001b[1;32m    111\u001b[0m     devset\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalset,\n\u001b[1;32m    112\u001b[0m     metric\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     display_progress\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    117\u001b[0m )\n\u001b[0;32m--> 119\u001b[0m score, subscores \u001b[39m=\u001b[39m evaluate(program, return_all_scores\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    121\u001b[0m all_subscores\u001b[39m.\u001b[39mappend(subscores)\n\u001b[1;32m    123\u001b[0m \u001b[39m############ Assertion-aware Optimization ############\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:160\u001b[0m, in \u001b[0;36mEvaluate.__call__\u001b[0;34m(self, program, metric, devset, num_threads, display_progress, display_table, return_all_scores, return_outputs)\u001b[0m\n\u001b[1;32m    156\u001b[0m         program\u001b[39m.\u001b[39m_suggest_failures \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m dspy\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39msuggest_failures\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m prediction, score\n\u001b[0;32m--> 160\u001b[0m results \u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39;49mexecute(process_item, devset)\n\u001b[1;32m    161\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(devset) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(results)\n\u001b[1;32m    163\u001b[0m results \u001b[39m=\u001b[39m [((dspy\u001b[39m.\u001b[39mPrediction(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfailure_score) \u001b[39mif\u001b[39;00m r \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m r) \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m results]\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/utils/parallelizer.py:39\u001b[0m, in \u001b[0;36mParallelExecutor.execute\u001b[0;34m(self, function, data)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_execute_isolated_single_thread(wrapped_function, data)\n\u001b[1;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_multi_thread(wrapped_function, data)\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/utils/parallelizer.py:180\u001b[0m, in \u001b[0;36mParallelExecutor._execute_multi_thread\u001b[0;34m(self, function, data)\u001b[0m\n\u001b[1;32m    172\u001b[0m pbar \u001b[39m=\u001b[39m tqdm\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m    173\u001b[0m     total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(data),\n\u001b[1;32m    174\u001b[0m     dynamic_ncols\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    175\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable_progress_bar,\n\u001b[1;32m    176\u001b[0m     file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mstdout\n\u001b[1;32m    177\u001b[0m )\n\u001b[1;32m    179\u001b[0m \u001b[39mfor\u001b[39;00m future \u001b[39min\u001b[39;00m as_completed(futures):\n\u001b[0;32m--> 180\u001b[0m     index, result \u001b[39m=\u001b[39m future\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    182\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m job_cancelled:\n\u001b[1;32m    183\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.11-linux-x86_64-gnu/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.11-linux-x86_64-gnu/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.11-linux-x86_64-gnu/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/utils/parallelizer.py:158\u001b[0m, in \u001b[0;36mParallelExecutor._execute_multi_thread.<locals>.cancellable_function\u001b[0;34m(parent_overrides, index_item)\u001b[0m\n\u001b[1;32m    155\u001b[0m thread_local_overrides\u001b[39m.\u001b[39moverrides \u001b[39m=\u001b[39m parent_overrides\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    157\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m index, function(item)\n\u001b[1;32m    159\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     thread_local_overrides\u001b[39m.\u001b[39moverrides \u001b[39m=\u001b[39m original_overrides\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/utils/parallelizer.py:54\u001b[0m, in \u001b[0;36mParallelExecutor._wrap_function.<locals>.wrapped\u001b[0;34m(item)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mif\u001b[39;00m current_error_count \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_errors:\n\u001b[1;32m     53\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_jobs\u001b[39m.\u001b[39mset()\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprovide_traceback:\n\u001b[1;32m     56\u001b[0m     logger\u001b[39m.\u001b[39merror(\n\u001b[1;32m     57\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError processing item \u001b[39m\u001b[39m{\u001b[39;00mitem\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mStack trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mtraceback\u001b[39m.\u001b[39mformat_exc()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m     )\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/utils/parallelizer.py:47\u001b[0m, in \u001b[0;36mParallelExecutor._wrap_function.<locals>.wrapped\u001b[0;34m(item)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mreturn\u001b[39;00m function(item)\n\u001b[1;32m     48\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     49\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_lock:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:149\u001b[0m, in \u001b[0;36mEvaluate.__call__.<locals>.process_item\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mprocess_item\u001b[39m(example):\n\u001b[0;32m--> 149\u001b[0m     prediction \u001b[39m=\u001b[39m program(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mexample\u001b[39m.\u001b[39;49minputs())\n\u001b[1;32m    150\u001b[0m     score \u001b[39m=\u001b[39m metric(example, prediction)\n\u001b[1;32m    152\u001b[0m     \u001b[39m# Increment assert and suggest failures to program's attributes\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/utils/callback.py:234\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39m# If no callbacks are provided, just call the function\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(instance, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    236\u001b[0m \u001b[39m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m call_id \u001b[39m=\u001b[39m uuid\u001b[39m.\u001b[39muuid4()\u001b[39m.\u001b[39mhex\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/predict/predict.py:67\u001b[0m, in \u001b[0;36mPredict.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m@with_callbacks\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/predict/predict.py:97\u001b[0m, in \u001b[0;36mPredict.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdspy\u001b[39;00m\n\u001b[1;32m     96\u001b[0m adapter \u001b[39m=\u001b[39m dspy\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39madapter \u001b[39mor\u001b[39;00m dspy\u001b[39m.\u001b[39mChatAdapter()\n\u001b[0;32m---> 97\u001b[0m completions \u001b[39m=\u001b[39m adapter(lm, lm_kwargs\u001b[39m=\u001b[39;49mconfig, signature\u001b[39m=\u001b[39;49msignature, demos\u001b[39m=\u001b[39;49mdemos, inputs\u001b[39m=\u001b[39;49mkwargs)\n\u001b[1;32m     99\u001b[0m pred \u001b[39m=\u001b[39m Prediction\u001b[39m.\u001b[39mfrom_completions(completions, signature\u001b[39m=\u001b[39msignature)\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_trace\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m) \u001b[39mand\u001b[39;00m dspy\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39mtrace \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/adapters/base.py:51\u001b[0m, in \u001b[0;36mAdapter.__call__\u001b[0;34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m.\u001b[39;00m\u001b[39mjson_adapter\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m JSONAdapter\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, JSONAdapter):\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m JSONAdapter()(lm, lm_kwargs, signature, demos, inputs)\n\u001b[1;32m     52\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/adapters/json_adapter.py:51\u001b[0m, in \u001b[0;36mJSONAdapter.__call__\u001b[0;34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m     47\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFailed to obtain response using signature-based structured outputs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m response format: Falling back to default \u001b[39m\u001b[39m'\u001b[39m\u001b[39mjson_object\u001b[39m\u001b[39m'\u001b[39m\u001b[39m response format.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m Exception: \u001b[39m\u001b[39m{e}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m         )\n\u001b[0;32m---> 51\u001b[0m         outputs \u001b[39m=\u001b[39m lm(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mlm_kwargs, response_format\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mjson_object\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     outputs \u001b[39m=\u001b[39m lm(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mlm_kwargs)\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/utils/callback.py:234\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39m# If no callbacks are provided, just call the function\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(instance, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    236\u001b[0m \u001b[39m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m call_id \u001b[39m=\u001b[39m uuid\u001b[39m.\u001b[39muuid4()\u001b[39m.\u001b[39mhex\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/clients/lm.py:119\u001b[0m, in \u001b[0;36mLM.__call__\u001b[0;34m(self, prompt, messages, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     completion \u001b[39m=\u001b[39m litellm_completion \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mchat\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m litellm_text_completion\n\u001b[0;32m--> 119\u001b[0m     response \u001b[39m=\u001b[39m completion(\n\u001b[1;32m    120\u001b[0m         request\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, messages\u001b[39m=\u001b[39;49mmessages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs),\n\u001b[1;32m    121\u001b[0m         num_retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_retries,\n\u001b[1;32m    122\u001b[0m         \u001b[39m# only leverage LiteLLM cache in this case\u001b[39;49;00m\n\u001b[1;32m    123\u001b[0m         cache\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mno-cache\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mnot\u001b[39;49;00m cache, \u001b[39m\"\u001b[39;49m\u001b[39mno-store\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mnot\u001b[39;49;00m cache},\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    127\u001b[0m     outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    128\u001b[0m         {\n\u001b[1;32m    129\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: c\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(c, \u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m c[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    133\u001b[0m     ]\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/clients/lm.py:350\u001b[0m, in \u001b[0;36mlitellm_completion\u001b[0;34m(request, num_retries, cache)\u001b[0m\n\u001b[1;32m    348\u001b[0m stream \u001b[39m=\u001b[39m dspy\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39msend_stream\n\u001b[1;32m    349\u001b[0m \u001b[39mif\u001b[39;00m stream \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 350\u001b[0m     \u001b[39mreturn\u001b[39;00m litellm\u001b[39m.\u001b[39;49mcompletion(\n\u001b[1;32m    351\u001b[0m         cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m    352\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mretry_kwargs,\n\u001b[1;32m    353\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest,\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    356\u001b[0m \u001b[39m# The stream is already opened, and will be closed by the caller.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m stream \u001b[39m=\u001b[39m cast(MemoryObjectSendStream, stream)\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/utils.py:1190\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39mif\u001b[39;00m logging_obj:\n\u001b[1;32m   1187\u001b[0m     logging_obj\u001b[39m.\u001b[39mfailure_handler(\n\u001b[1;32m   1188\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1189\u001b[0m     )  \u001b[39m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/utils.py:1068\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         print_verbose(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError while checking max token limit: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1067\u001b[0m \u001b[39m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1068\u001b[0m result \u001b[39m=\u001b[39m original_function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1069\u001b[0m end_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m   1070\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m kwargs[\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/main.py:3084\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   3081\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[1;32m   3082\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   3083\u001b[0m     \u001b[39m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 3084\u001b[0m     \u001b[39mraise\u001b[39;00m exception_type(\n\u001b[1;32m   3085\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   3086\u001b[0m         custom_llm_provider\u001b[39m=\u001b[39;49mcustom_llm_provider,\n\u001b[1;32m   3087\u001b[0m         original_exception\u001b[39m=\u001b[39;49me,\n\u001b[1;32m   3088\u001b[0m         completion_kwargs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   3089\u001b[0m         extra_kwargs\u001b[39m=\u001b[39;49mkwargs,\n\u001b[1;32m   3090\u001b[0m     )\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2202\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2200\u001b[0m \u001b[39mif\u001b[39;00m exception_mapping_worked:\n\u001b[1;32m   2201\u001b[0m     \u001b[39msetattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mlitellm_response_headers\u001b[39m\u001b[39m\"\u001b[39m, litellm_response_headers)\n\u001b[0;32m-> 2202\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   2203\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2204\u001b[0m     \u001b[39mfor\u001b[39;00m error_type \u001b[39min\u001b[39;00m litellm\u001b[39m.\u001b[39mLITELLM_EXCEPTION_TYPES:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:382\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m original_exception\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m400\u001b[39m:\n\u001b[1;32m    381\u001b[0m     exception_mapping_worked \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m     \u001b[39mraise\u001b[39;00m BadRequestError(\n\u001b[1;32m    383\u001b[0m         message\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mexception_provider\u001b[39m}\u001b[39;00m\u001b[39m - \u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    384\u001b[0m         llm_provider\u001b[39m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m    385\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    386\u001b[0m         response\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(original_exception, \u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    387\u001b[0m         litellm_debug_info\u001b[39m=\u001b[39mextra_information,\n\u001b[1;32m    388\u001b[0m     )\n\u001b[1;32m    389\u001b[0m \u001b[39melif\u001b[39;00m original_exception\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m401\u001b[39m:\n\u001b[1;32m    390\u001b[0m     exception_mapping_worked \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': {'message': 'litellm.BadRequestError: OpenAIException - Failed to deserialize the JSON body into the target type: response_format: missing field `value` at line 1 column 9585\\nReceived Model Group=llama-3-8b\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '400'}}"
     ]
    }
   ],
   "source": [
    "optimizer_config = {\n",
    "    \"class\": \"BootstrapFewShotWithRandomSearch\",\n",
    "    \"params\": {\n",
    "        \"max_bootstrapped_demos\": 8,\n",
    "        \"max_labeled_demos\": 8,\n",
    "        \"max_rounds\": 16,\n",
    "        \"num_candidate_programs\": 16,\n",
    "        \"num_threads\": 16,\n",
    "        \"max_errors\": 10,\n",
    "        \"metric_threshold\": None,\n",
    "        \"stop_at_score\": None,\n",
    "    },\n",
    "    \"with_metric\": True,\n",
    "}\n",
    "\n",
    "train(\n",
    "    dataset_path=dataset_path,\n",
    "    dataset_name=dataset_name,\n",
    "    dataset_split=\"train[:100]\",\n",
    "    model=\"llama-3-8b\",\n",
    "    temperature=0.0,\n",
    "    optimizer_config=optimizer_config,\n",
    "    out=\"../tmp/erx/dspy/trained-program.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 39.40 / 100 (39.4%): 100%|| 100/100 [00:18<00:00,  5.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:10:40 INFO dspy.evaluate.evaluate: Average Metric: 39.40046495393733 / 100 (39.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results, scores = evaluate(\n",
    "    dataset_path=dataset_path,\n",
    "    dataset_name=dataset_name,\n",
    "    dataset_split=\"dev[:100]\",\n",
    "    model=\"llama-3-8b\",\n",
    "    temperature=0.0,\n",
    "    out_dir=\"../tmp/erx/dspy/after\",\n",
    "    load_from=\"../tmp/erx/dspy/trained-program.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
