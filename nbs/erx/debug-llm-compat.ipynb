{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import dspy\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "def jprint(obj):\n",
    "    print(json.dumps(obj, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"properties\": {\n",
      "    \"triples\": {\n",
      "      \"description\": \"The `subject | predicate | object` triples extracted from the text.\",\n",
      "      \"items\": {\n",
      "        \"type\": \"string\"\n",
      "      },\n",
      "      \"title\": \"Triples\",\n",
      "      \"type\": \"array\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"triples\"\n",
      "  ],\n",
      "  \"title\": \"EntityRelationExtraction\",\n",
      "  \"type\": \"object\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "class EntityRelationExtraction(BaseModel):\n",
    "    triples: list[str] = Field(description=\"The `subject | predicate | object` triples extracted from the text.\")\n",
    "\n",
    "schema = EntityRelationExtraction.model_json_schema()\n",
    "jprint(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = dspy.LM(\n",
    "#     \"openai/llama-3-8b\",\n",
    "#     temperature=0,\n",
    "#     cache=False,\n",
    "#     api_base=\"http://0.0.0.0:8008/v1\",\n",
    "#     api_key=\"tgi\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SFT\n",
    "# lm = dspy.LM(\n",
    "#     \"openai/llama-3-8b\",\n",
    "#     temperature=0,\n",
    "#     cache=False,\n",
    "#     api_base=\"http://0.0.0.0:8208/v1\",\n",
    "#     api_key=\"tgi\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = dspy.LM(\n",
    "#     \"openai/qwen-2.5-32b\",\n",
    "#     temperature=0,\n",
    "#     cache=False,\n",
    "#     api_base=os.getenv(\"GROQ_API_BASE\"),\n",
    "#     api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\n",
    "    \"groq/llama-3.1-8b-instant\",\n",
    "    temperature=0,\n",
    "    cache=False,\n",
    "    api_base=os.getenv(\"GROQ_API_BASE\"),\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = dspy.LM(\n",
    "#     \"hosted_vllm/llama-3-8b\",\n",
    "#     temperature=0,\n",
    "#     cache=False,\n",
    "#     api_base=\"http://0.0.0.0:8008/v1\",\n",
    "#     api_key=\"tgi\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That\\'s correct. \\n\\n1. Ankara is indeed the capital of Turkey. It has been the capital since 1923, when the Turkish government moved from Istanbul.\\n\\n2. Claude Shannon is widely regarded as the father of information theory. He was an American mathematician, electrical engineer, and cryptographer who made significant contributions to the field of information theory. His 1948 paper \"A Mathematical Theory of Communication\" laid the foundation for modern information theory and has had a lasting impact on the development of digital communication systems.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Ankara is the capital of Turkey.\n",
    "Claude Shannon is the father of information theory.\n",
    "\"\"\".strip()\n",
    "response = lm(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ],\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions + schema but no response format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the extracted entities and relations in JSON format:\n",
      "\n",
      "{\n",
      "  \"properties\": {\n",
      "    \"triples\": {\n",
      "      \"description\": \"The `subject | predicate | object` triples extracted from the text.\",\n",
      "      \"items\": {\n",
      "        \"type\": \"string\"\n",
      "      },\n",
      "      \"title\": \"Triples\",\n",
      "      \"type\": \"array\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"triples\"\n",
      "  ],\n",
      "  \"title\": \"EntityRelationExtraction\",\n",
      "  \"type\": \"object\",\n",
      "  \"triples\": [\n",
      "    {\n",
      "      \"object\": \"Turkey\",\n",
      "      \"predicate\": \"capital of\",\n",
      "      \"subject\": \"Ankara\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"information theory\",\n",
      "      \"predicate\": \"father of\",\n",
      "      \"subject\": \"Claude Shannon\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Ankara is the capital of Turkey.\n",
    "Claude Shannon is the father of information theory.\n",
    "\"\"\".strip()\n",
    "response = lm(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are an entity relation extraction model. You will be given a text and you will need to extract the entities and relations from the text.\\n\\nJSON Schema:\\n{schema}\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions + schema + response format = json_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 validation errors for EntityRelationExtraction\n",
      "triples.0\n",
      "  Input should be a valid string [type=string_type, input_value=['Ankara', 'is the capital of', 'Turkey'], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "triples.1\n",
      "  Input should be a valid string [type=string_type, input_value=['Claude Shannon', 'is th...', 'information theory'], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "{\n",
      "   \"triples\": [\n",
      "      [\n",
      "         \"Ankara\",\n",
      "         \"is the capital of\",\n",
      "         \"Turkey\"\n",
      "      ],\n",
      "      [\n",
      "         \"Claude Shannon\",\n",
      "         \"is the father of\",\n",
      "         \"information theory\"\n",
      "      ]\n",
      "   ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Ankara is the capital of Turkey.\n",
    "Claude Shannon is the father of information theory.\n",
    "\"\"\".strip()\n",
    "response = lm(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are an entity relation extraction model. You will be given a text and you will need to extract the entities and relations from the text.\\n\\nJSON Schema:\\n{schema}\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\", \"value\": schema},\n",
    ")\n",
    "try:\n",
    "    output = EntityRelationExtraction.model_validate_json(response[0])\n",
    "    print(repr(output))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions + response format = Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntityRelationExtraction(triples=['Ankara, is the capital of, Turkey.', 'Claude Shannon, is the father of, information theory.'])\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Ankara is the capital of Turkey.\n",
    "Claude Shannon is the father of information theory.\n",
    "\"\"\".strip()\n",
    "response = lm(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an entity relation extraction model. You will be given a text and you will need to extract the entities and relations from the text.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ],\n",
    "    response_format=EntityRelationExtraction,\n",
    ")\n",
    "\n",
    "try:\n",
    "    output = EntityRelationExtraction.model_validate_json(response[0])\n",
    "    print(repr(output))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: GroqException - {\"error\":{\"message\":\"'tool_choice' : value must be a string (auto or none) or an object like `{'type': 'function', 'function': {'name': 'my_function'}}`\",\"type\":\"invalid_request_error\"}}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai_like/chat/handler.py:372\u001b[0m, in \u001b[0;36mOpenAILikeChatHandler.completion\u001b[0;34m(self, model, messages, api_base, custom_llm_provider, custom_prompt_dict, model_response, print_verbose, encoding, api_key, logging_obj, optional_params, acompletion, litellm_params, logger_fn, headers, timeout, client, custom_endpoint, streaming_decoder, fake_stream)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 372\u001b[0m     response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mpost(\n\u001b[1;32m    373\u001b[0m         url\u001b[39m=\u001b[39;49mapi_base, headers\u001b[39m=\u001b[39;49mheaders, data\u001b[39m=\u001b[39;49mjson\u001b[39m.\u001b[39;49mdumps(data)\n\u001b[1;32m    374\u001b[0m     )\n\u001b[1;32m    375\u001b[0m     response\u001b[39m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/custom_httpx/http_handler.py:557\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[0;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[39msetattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mstatus_code\u001b[39m\u001b[39m\"\u001b[39m, e\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mstatus_code)\n\u001b[0;32m--> 557\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    558\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/custom_httpx/http_handler.py:538\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[0;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[1;32m    537\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39msend(req, stream\u001b[39m=\u001b[39mstream)\n\u001b[0;32m--> 538\u001b[0m response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    539\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/httpx/_models.py:829\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m message \u001b[39m=\u001b[39m message\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m, error_type\u001b[39m=\u001b[39merror_type)\n\u001b[0;32m--> 829\u001b[0m \u001b[39mraise\u001b[39;00m HTTPStatusError(message, request\u001b[39m=\u001b[39mrequest, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '400 Bad Request' for url 'https://api.groq.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOpenAILikeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/main.py:1581\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m             optional_params[k] \u001b[39m=\u001b[39m v\n\u001b[0;32m-> 1581\u001b[0m     response \u001b[39m=\u001b[39m groq_chat_completions\u001b[39m.\u001b[39;49mcompletion(\n\u001b[1;32m   1582\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1583\u001b[0m         messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m   1584\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1585\u001b[0m         model_response\u001b[39m=\u001b[39;49mmodel_response,\n\u001b[1;32m   1586\u001b[0m         print_verbose\u001b[39m=\u001b[39;49mprint_verbose,\n\u001b[1;32m   1587\u001b[0m         api_key\u001b[39m=\u001b[39;49mapi_key,\n\u001b[1;32m   1588\u001b[0m         api_base\u001b[39m=\u001b[39;49mapi_base,\n\u001b[1;32m   1589\u001b[0m         acompletion\u001b[39m=\u001b[39;49macompletion,\n\u001b[1;32m   1590\u001b[0m         logging_obj\u001b[39m=\u001b[39;49mlogging,\n\u001b[1;32m   1591\u001b[0m         optional_params\u001b[39m=\u001b[39;49moptional_params,\n\u001b[1;32m   1592\u001b[0m         litellm_params\u001b[39m=\u001b[39;49mlitellm_params,\n\u001b[1;32m   1593\u001b[0m         logger_fn\u001b[39m=\u001b[39;49mlogger_fn,\n\u001b[1;32m   1594\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   1595\u001b[0m         custom_prompt_dict\u001b[39m=\u001b[39;49mcustom_prompt_dict,\n\u001b[1;32m   1596\u001b[0m         client\u001b[39m=\u001b[39;49mclient,  \u001b[39m# pass AsyncOpenAI, OpenAI client\u001b[39;49;00m\n\u001b[1;32m   1597\u001b[0m         custom_llm_provider\u001b[39m=\u001b[39;49mcustom_llm_provider,\n\u001b[1;32m   1598\u001b[0m         encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   1599\u001b[0m     )\n\u001b[1;32m   1600\u001b[0m \u001b[39melif\u001b[39;00m custom_llm_provider \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39maiohttp_openai\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1601\u001b[0m     \u001b[39m# NEW aiohttp provider for 10-100x higher RPS\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/groq/chat/handler.py:55\u001b[0m, in \u001b[0;36mGroqChatCompletion.completion\u001b[0;34m(self, model, messages, api_base, custom_llm_provider, custom_prompt_dict, model_response, print_verbose, encoding, api_key, logging_obj, optional_params, acompletion, litellm_params, logger_fn, headers, timeout, client, custom_endpoint, streaming_decoder, fake_stream)\u001b[0m\n\u001b[1;32m     53\u001b[0m     fake_stream \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcompletion(\n\u001b[1;32m     56\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     57\u001b[0m     messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m     58\u001b[0m     api_base\u001b[39m=\u001b[39;49mapi_base,\n\u001b[1;32m     59\u001b[0m     custom_llm_provider\u001b[39m=\u001b[39;49mcustom_llm_provider,\n\u001b[1;32m     60\u001b[0m     custom_prompt_dict\u001b[39m=\u001b[39;49mcustom_prompt_dict,\n\u001b[1;32m     61\u001b[0m     model_response\u001b[39m=\u001b[39;49mmodel_response,\n\u001b[1;32m     62\u001b[0m     print_verbose\u001b[39m=\u001b[39;49mprint_verbose,\n\u001b[1;32m     63\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m     64\u001b[0m     api_key\u001b[39m=\u001b[39;49mapi_key,\n\u001b[1;32m     65\u001b[0m     logging_obj\u001b[39m=\u001b[39;49mlogging_obj,\n\u001b[1;32m     66\u001b[0m     optional_params\u001b[39m=\u001b[39;49moptional_params,\n\u001b[1;32m     67\u001b[0m     acompletion\u001b[39m=\u001b[39;49macompletion,\n\u001b[1;32m     68\u001b[0m     litellm_params\u001b[39m=\u001b[39;49mlitellm_params,\n\u001b[1;32m     69\u001b[0m     logger_fn\u001b[39m=\u001b[39;49mlogger_fn,\n\u001b[1;32m     70\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m     71\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m     72\u001b[0m     client\u001b[39m=\u001b[39;49mclient,\n\u001b[1;32m     73\u001b[0m     custom_endpoint\u001b[39m=\u001b[39;49mcustom_endpoint,\n\u001b[1;32m     74\u001b[0m     streaming_decoder\u001b[39m=\u001b[39;49mstreaming_decoder,\n\u001b[1;32m     75\u001b[0m     fake_stream\u001b[39m=\u001b[39;49mfake_stream,\n\u001b[1;32m     76\u001b[0m )\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai_like/chat/handler.py:378\u001b[0m, in \u001b[0;36mOpenAILikeChatHandler.completion\u001b[0;34m(self, model, messages, api_base, custom_llm_provider, custom_prompt_dict, model_response, print_verbose, encoding, api_key, logging_obj, optional_params, acompletion, litellm_params, logger_fn, headers, timeout, client, custom_endpoint, streaming_decoder, fake_stream)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mHTTPStatusError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 378\u001b[0m     \u001b[39mraise\u001b[39;00m OpenAILikeError(\n\u001b[1;32m    379\u001b[0m         status_code\u001b[39m=\u001b[39me\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mstatus_code,\n\u001b[1;32m    380\u001b[0m         message\u001b[39m=\u001b[39me\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mtext,\n\u001b[1;32m    381\u001b[0m     )\n\u001b[1;32m    382\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException:\n",
      "\u001b[0;31mOpenAILikeError\u001b[0m: {\"error\":{\"message\":\"'tool_choice' : value must be a string (auto or none) or an object like `{'type': 'function', 'function': {'name': 'my_function'}}`\",\"type\":\"invalid_request_error\"}}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb Cell 18\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mAnkara is the capital of Turkey.\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mClaude Shannon is the father of information theory.\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m response \u001b[39m=\u001b[39m lm(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         {\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mYou are an entity relation extraction model. You will be given a text and you will need to extract the entities and relations from the text.\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mJSON Schema:\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m{\u001b[39;49;00mschema\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         },\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: text},\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     ],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     tools\u001b[39m=\u001b[39;49m [\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         {\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mfunction\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mfunction\u001b[39;49m\u001b[39m\"\u001b[39;49m: {\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msave\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mdescription\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mSave extracted triples\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mparameters\u001b[39;49m\u001b[39m\"\u001b[39;49m: schema,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m             }\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m         }\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     ],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     tool_choice\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msave\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m response\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/utils/callback.py:234\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39m# If no callbacks are provided, just call the function\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(instance, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    236\u001b[0m \u001b[39m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m call_id \u001b[39m=\u001b[39m uuid\u001b[39m.\u001b[39muuid4()\u001b[39m.\u001b[39mhex\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/clients/lm.py:119\u001b[0m, in \u001b[0;36mLM.__call__\u001b[0;34m(self, prompt, messages, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     completion \u001b[39m=\u001b[39m litellm_completion \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mchat\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m litellm_text_completion\n\u001b[0;32m--> 119\u001b[0m     response \u001b[39m=\u001b[39m completion(\n\u001b[1;32m    120\u001b[0m         request\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, messages\u001b[39m=\u001b[39;49mmessages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs),\n\u001b[1;32m    121\u001b[0m         num_retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_retries,\n\u001b[1;32m    122\u001b[0m         \u001b[39m# only leverage LiteLLM cache in this case\u001b[39;49;00m\n\u001b[1;32m    123\u001b[0m         cache\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mno-cache\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mnot\u001b[39;49;00m cache, \u001b[39m\"\u001b[39;49m\u001b[39mno-store\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mnot\u001b[39;49;00m cache},\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    127\u001b[0m     outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    128\u001b[0m         {\n\u001b[1;32m    129\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: c\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(c, \u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m c[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    133\u001b[0m     ]\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/clients/lm.py:350\u001b[0m, in \u001b[0;36mlitellm_completion\u001b[0;34m(request, num_retries, cache)\u001b[0m\n\u001b[1;32m    348\u001b[0m stream \u001b[39m=\u001b[39m dspy\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39msend_stream\n\u001b[1;32m    349\u001b[0m \u001b[39mif\u001b[39;00m stream \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 350\u001b[0m     \u001b[39mreturn\u001b[39;00m litellm\u001b[39m.\u001b[39;49mcompletion(\n\u001b[1;32m    351\u001b[0m         cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m    352\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mretry_kwargs,\n\u001b[1;32m    353\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest,\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    356\u001b[0m \u001b[39m# The stream is already opened, and will be closed by the caller.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m stream \u001b[39m=\u001b[39m cast(MemoryObjectSendStream, stream)\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/utils.py:1190\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39mif\u001b[39;00m logging_obj:\n\u001b[1;32m   1187\u001b[0m     logging_obj\u001b[39m.\u001b[39mfailure_handler(\n\u001b[1;32m   1188\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1189\u001b[0m     )  \u001b[39m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/utils.py:1068\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         print_verbose(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError while checking max token limit: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1067\u001b[0m \u001b[39m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1068\u001b[0m result \u001b[39m=\u001b[39m original_function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1069\u001b[0m end_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m   1070\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m kwargs[\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/main.py:3084\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   3081\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[1;32m   3082\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   3083\u001b[0m     \u001b[39m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 3084\u001b[0m     \u001b[39mraise\u001b[39;00m exception_type(\n\u001b[1;32m   3085\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   3086\u001b[0m         custom_llm_provider\u001b[39m=\u001b[39;49mcustom_llm_provider,\n\u001b[1;32m   3087\u001b[0m         original_exception\u001b[39m=\u001b[39;49me,\n\u001b[1;32m   3088\u001b[0m         completion_kwargs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   3089\u001b[0m         extra_kwargs\u001b[39m=\u001b[39;49mkwargs,\n\u001b[1;32m   3090\u001b[0m     )\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2202\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2200\u001b[0m \u001b[39mif\u001b[39;00m exception_mapping_worked:\n\u001b[1;32m   2201\u001b[0m     \u001b[39msetattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mlitellm_response_headers\u001b[39m\u001b[39m\"\u001b[39m, litellm_response_headers)\n\u001b[0;32m-> 2202\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   2203\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2204\u001b[0m     \u001b[39mfor\u001b[39;00m error_type \u001b[39min\u001b[39;00m litellm\u001b[39m.\u001b[39mLITELLM_EXCEPTION_TYPES:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:327\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[1;32m    323\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minvalid_request_error\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m error_str\n\u001b[1;32m    324\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mIncorrect API key provided\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m error_str\n\u001b[1;32m    325\u001b[0m ):\n\u001b[1;32m    326\u001b[0m     exception_mapping_worked \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     \u001b[39mraise\u001b[39;00m BadRequestError(\n\u001b[1;32m    328\u001b[0m         message\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mexception_provider\u001b[39m}\u001b[39;00m\u001b[39m - \u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    329\u001b[0m         llm_provider\u001b[39m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m    330\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    331\u001b[0m         response\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(original_exception, \u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    332\u001b[0m         litellm_debug_info\u001b[39m=\u001b[39mextra_information,\n\u001b[1;32m    333\u001b[0m     )\n\u001b[1;32m    334\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[1;32m    335\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mWeb server is returning an unknown error\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m error_str\n\u001b[1;32m    336\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mThe server had an error processing your request.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m error_str\n\u001b[1;32m    337\u001b[0m ):\n\u001b[1;32m    338\u001b[0m     exception_mapping_worked \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: litellm.BadRequestError: GroqException - {\"error\":{\"message\":\"'tool_choice' : value must be a string (auto or none) or an object like `{'type': 'function', 'function': {'name': 'my_function'}}`\",\"type\":\"invalid_request_error\"}}\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Ankara is the capital of Turkey.\n",
    "Claude Shannon is the father of information theory.\n",
    "\"\"\".strip()\n",
    "response = lm(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are an entity relation extraction model. You will be given a text and you will need to extract the entities and relations from the text.\\n\\nJSON Schema:\\n{schema}\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ],\n",
    "    tools= [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"save\",\n",
    "                \"description\": \"Save extracted triples\",\n",
    "                \"parameters\": schema,\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    tool_choice=\"save\"\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
