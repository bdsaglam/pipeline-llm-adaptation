{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'triples': {'description': 'The `subject | predicate | object` triples extracted from the text.',\n",
       "   'items': {'type': 'string'},\n",
       "   'title': 'Triples',\n",
       "   'type': 'array'}},\n",
       " 'required': ['triples'],\n",
       " 'title': 'EntityRelationExtraction',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class EntityRelationExtraction(BaseModel):\n",
    "    triples: list[str] = Field(description=\"The `subject | predicate | object` triples extracted from the text.\")\n",
    "\n",
    "schema = EntityRelationExtraction.model_json_schema()\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = dspy.LM(\n",
    "#     \"openai/llama-3-8b\",\n",
    "#     temperature=0,\n",
    "#     cache=False,\n",
    "#     api_base=\"http://0.0.0.0:8008/v1\",\n",
    "#     api_key=\"tgi\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT\n",
    "lm = dspy.LM(\n",
    "    \"openai/llama-3-8b\",\n",
    "    temperature=0,\n",
    "    cache=False,\n",
    "    api_base=\"http://0.0.0.0:8208/v1\",\n",
    "    api_key=\"tgi\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = dspy.LM(\n",
    "#     \"openai/qwen-2.5-32b\",\n",
    "#     temperature=0,\n",
    "#     cache=False,\n",
    "#     api_base=os.getenv(\"GROQ_API_BASE\"),\n",
    "#     api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = dspy.LM(\n",
    "#     \"hosted_vllm/llama-3-8b\",\n",
    "#     temperature=0,\n",
    "#     cache=False,\n",
    "#     api_base=\"http://0.0.0.0:8008/v1\",\n",
    "#     api_key=\"tgi\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ankara | capital | Turkey\\nClaude Shannon | field | Information theory\\nClaude Shannon | occupation | Father of information theory']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Ankara is the capital of Turkey.\n",
    "Claude Shannon is the father of information theory.\n",
    "\"\"\".strip()\n",
    "response = lm(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ],\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntityRelationExtraction(triples=['Ankara | is the capital of | Turkey', 'Claude Shannon | is the father of | information theory'])\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Ankara is the capital of Turkey.\n",
    "Claude Shannon is the father of information theory.\n",
    "\"\"\".strip()\n",
    "response = lm(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are an entity relation extraction model. You will be given a text and you will need to extract the entities and relations from the text.\\n\\nJSON Schema:\\n{schema}\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\", \"value\": schema},\n",
    ")\n",
    "\n",
    "try:\n",
    "    output = EntityRelationExtraction.model_validate_json(response[0])\n",
    "    print(repr(output))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: OpenAIException - Failed to deserialize the JSON body into the target type: response_format.type: unknown variant `json_schema`, expected one of `json`, `json_object`, `regex` at line 1 column 656",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnprocessableEntityError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:691\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 691\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    692\u001b[0m \u001b[39m# e.message\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:653\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    641\u001b[0m logging_obj\u001b[39m.\u001b[39mpre_call(\n\u001b[1;32m    642\u001b[0m     \u001b[39minput\u001b[39m\u001b[39m=\u001b[39mmessages,\n\u001b[1;32m    643\u001b[0m     api_key\u001b[39m=\u001b[39mopenai_client\u001b[39m.\u001b[39mapi_key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m     },\n\u001b[1;32m    650\u001b[0m )\n\u001b[1;32m    652\u001b[0m headers, response \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 653\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_sync_openai_chat_completion_request(\n\u001b[1;32m    654\u001b[0m         openai_client\u001b[39m=\u001b[39;49mopenai_client,\n\u001b[1;32m    655\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    656\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    657\u001b[0m         logging_obj\u001b[39m=\u001b[39;49mlogging_obj,\n\u001b[1;32m    658\u001b[0m     )\n\u001b[1;32m    659\u001b[0m )\n\u001b[1;32m    661\u001b[0m logging_obj\u001b[39m.\u001b[39mmodel_call_details[\u001b[39m\"\u001b[39m\u001b[39mresponse_headers\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m headers\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_utils.py:145\u001b[0m, in \u001b[0;36mtrack_llm_api_timing.<locals>.decorator.<locals>.sync_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    146\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:472\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[0;34m(self, openai_client, data, timeout, logging_obj)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 472\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:454\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[0;34m(self, openai_client, data, timeout, logging_obj)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     raw_response \u001b[39m=\u001b[39m openai_client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mwith_raw_response\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    455\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdata, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    456\u001b[0m     )\n\u001b[1;32m    458\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(raw_response, \u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mextra_headers\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[39mreturn\u001b[39;00m cast(LegacyAPIResponse[R], func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:879\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    878\u001b[0m validate_response_format(response_format)\n\u001b[0;32m--> 879\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    880\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    881\u001b[0m     body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    882\u001b[0m         {\n\u001b[1;32m    883\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    884\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    885\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m\"\u001b[39;49m: audio,\n\u001b[1;32m    886\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    887\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    888\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    889\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    890\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    891\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmax_completion_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_completion_tokens,\n\u001b[1;32m    892\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    893\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m: metadata,\n\u001b[1;32m    894\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmodalities\u001b[39;49m\u001b[39m\"\u001b[39;49m: modalities,\n\u001b[1;32m    895\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    896\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mparallel_tool_calls\u001b[39;49m\u001b[39m\"\u001b[39;49m: parallel_tool_calls,\n\u001b[1;32m    897\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mprediction\u001b[39;49m\u001b[39m\"\u001b[39;49m: prediction,\n\u001b[1;32m    898\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    899\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mreasoning_effort\u001b[39;49m\u001b[39m\"\u001b[39;49m: reasoning_effort,\n\u001b[1;32m    900\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    901\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    902\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mservice_tier\u001b[39;49m\u001b[39m\"\u001b[39;49m: service_tier,\n\u001b[1;32m    903\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    904\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstore\u001b[39;49m\u001b[39m\"\u001b[39;49m: store,\n\u001b[1;32m    905\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    906\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstream_options\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream_options,\n\u001b[1;32m    907\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    908\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    909\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    910\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[1;32m    911\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    912\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    913\u001b[0m         },\n\u001b[1;32m    914\u001b[0m         completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    915\u001b[0m     ),\n\u001b[1;32m    916\u001b[0m     options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    917\u001b[0m         extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    918\u001b[0m     ),\n\u001b[1;32m    919\u001b[0m     cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    920\u001b[0m     stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    921\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    922\u001b[0m )\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/_base_client.py:1290\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1287\u001b[0m opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1288\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1289\u001b[0m )\n\u001b[0;32m-> 1290\u001b[0m \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/_base_client.py:967\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    965\u001b[0m     retries_taken \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 967\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    968\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    969\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    970\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    971\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    972\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m    973\u001b[0m )\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/_base_client.py:1071\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1071\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[1;32m   1074\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1075\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1079\u001b[0m     retries_taken\u001b[39m=\u001b[39mretries_taken,\n\u001b[1;32m   1080\u001b[0m )\n",
      "\u001b[0;31mUnprocessableEntityError\u001b[0m: Failed to deserialize the JSON body into the target type: response_format.type: unknown variant `json_schema`, expected one of `json`, `json_object`, `regex` at line 1 column 656",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/main.py:1723\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     logging\u001b[39m.\u001b[39mpost_call(\n\u001b[1;32m   1718\u001b[0m         \u001b[39minput\u001b[39m\u001b[39m=\u001b[39mmessages,\n\u001b[1;32m   1719\u001b[0m         api_key\u001b[39m=\u001b[39mapi_key,\n\u001b[1;32m   1720\u001b[0m         original_response\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(e),\n\u001b[1;32m   1721\u001b[0m         additional_args\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m: headers},\n\u001b[1;32m   1722\u001b[0m     )\n\u001b[0;32m-> 1723\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   1725\u001b[0m \u001b[39mif\u001b[39;00m optional_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1726\u001b[0m     \u001b[39m## LOGGING\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/main.py:1696\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1695\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1696\u001b[0m     response \u001b[39m=\u001b[39m openai_chat_completions\u001b[39m.\u001b[39;49mcompletion(\n\u001b[1;32m   1697\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1698\u001b[0m         messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m   1699\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1700\u001b[0m         model_response\u001b[39m=\u001b[39;49mmodel_response,\n\u001b[1;32m   1701\u001b[0m         print_verbose\u001b[39m=\u001b[39;49mprint_verbose,\n\u001b[1;32m   1702\u001b[0m         api_key\u001b[39m=\u001b[39;49mapi_key,\n\u001b[1;32m   1703\u001b[0m         api_base\u001b[39m=\u001b[39;49mapi_base,\n\u001b[1;32m   1704\u001b[0m         acompletion\u001b[39m=\u001b[39;49macompletion,\n\u001b[1;32m   1705\u001b[0m         logging_obj\u001b[39m=\u001b[39;49mlogging,\n\u001b[1;32m   1706\u001b[0m         optional_params\u001b[39m=\u001b[39;49moptional_params,\n\u001b[1;32m   1707\u001b[0m         litellm_params\u001b[39m=\u001b[39;49mlitellm_params,\n\u001b[1;32m   1708\u001b[0m         logger_fn\u001b[39m=\u001b[39;49mlogger_fn,\n\u001b[1;32m   1709\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   1710\u001b[0m         custom_prompt_dict\u001b[39m=\u001b[39;49mcustom_prompt_dict,\n\u001b[1;32m   1711\u001b[0m         client\u001b[39m=\u001b[39;49mclient,  \u001b[39m# pass AsyncOpenAI, OpenAI client\u001b[39;49;00m\n\u001b[1;32m   1712\u001b[0m         organization\u001b[39m=\u001b[39;49morganization,\n\u001b[1;32m   1713\u001b[0m         custom_llm_provider\u001b[39m=\u001b[39;49mcustom_llm_provider,\n\u001b[1;32m   1714\u001b[0m     )\n\u001b[1;32m   1715\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1716\u001b[0m     \u001b[39m## LOGGING - log the original exception returned\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:736\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    735\u001b[0m     error_headers \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(error_response, \u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 736\u001b[0m \u001b[39mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    737\u001b[0m     status_code\u001b[39m=\u001b[39mstatus_code, message\u001b[39m=\u001b[39merror_text, headers\u001b[39m=\u001b[39merror_headers\n\u001b[1;32m    738\u001b[0m )\n",
      "\u001b[0;31mOpenAIError\u001b[0m: Failed to deserialize the JSON body into the target type: response_format.type: unknown variant `json_schema`, expected one of `json`, `json_object`, `regex` at line 1 column 656",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mAnkara is the capital of Turkey.\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mClaude Shannon is the father of information theory.\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m response \u001b[39m=\u001b[39m lm(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         {\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mYou are an entity relation extraction model. You will be given a text and you will need to extract the entities and relations from the text.\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mJSON Schema:\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m{\u001b[39;49;00mschema\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         },\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: text},\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     ],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     response_format\u001b[39m=\u001b[39;49m{ \u001b[39m\"\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mjson_schema\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mjson_schema\u001b[39;49m\u001b[39m\"\u001b[39;49m: schema , \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mTrue\u001b[39;49;00m }\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     output \u001b[39m=\u001b[39m EntityRelationExtraction\u001b[39m.\u001b[39mmodel_validate_json(response[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/utils/callback.py:234\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39m# If no callbacks are provided, just call the function\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(instance, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    236\u001b[0m \u001b[39m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m call_id \u001b[39m=\u001b[39m uuid\u001b[39m.\u001b[39muuid4()\u001b[39m.\u001b[39mhex\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/clients/lm.py:119\u001b[0m, in \u001b[0;36mLM.__call__\u001b[0;34m(self, prompt, messages, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     completion \u001b[39m=\u001b[39m litellm_completion \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mchat\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m litellm_text_completion\n\u001b[0;32m--> 119\u001b[0m     response \u001b[39m=\u001b[39m completion(\n\u001b[1;32m    120\u001b[0m         request\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, messages\u001b[39m=\u001b[39;49mmessages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs),\n\u001b[1;32m    121\u001b[0m         num_retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_retries,\n\u001b[1;32m    122\u001b[0m         \u001b[39m# only leverage LiteLLM cache in this case\u001b[39;49;00m\n\u001b[1;32m    123\u001b[0m         cache\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mno-cache\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mnot\u001b[39;49;00m cache, \u001b[39m\"\u001b[39;49m\u001b[39mno-store\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mnot\u001b[39;49;00m cache},\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    127\u001b[0m     outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    128\u001b[0m         {\n\u001b[1;32m    129\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: c\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(c, \u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m c[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    133\u001b[0m     ]\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/clients/lm.py:350\u001b[0m, in \u001b[0;36mlitellm_completion\u001b[0;34m(request, num_retries, cache)\u001b[0m\n\u001b[1;32m    348\u001b[0m stream \u001b[39m=\u001b[39m dspy\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39msend_stream\n\u001b[1;32m    349\u001b[0m \u001b[39mif\u001b[39;00m stream \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 350\u001b[0m     \u001b[39mreturn\u001b[39;00m litellm\u001b[39m.\u001b[39;49mcompletion(\n\u001b[1;32m    351\u001b[0m         cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m    352\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mretry_kwargs,\n\u001b[1;32m    353\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest,\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    356\u001b[0m \u001b[39m# The stream is already opened, and will be closed by the caller.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m stream \u001b[39m=\u001b[39m cast(MemoryObjectSendStream, stream)\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/utils.py:1190\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39mif\u001b[39;00m logging_obj:\n\u001b[1;32m   1187\u001b[0m     logging_obj\u001b[39m.\u001b[39mfailure_handler(\n\u001b[1;32m   1188\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1189\u001b[0m     )  \u001b[39m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/utils.py:1068\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         print_verbose(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError while checking max token limit: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1067\u001b[0m \u001b[39m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1068\u001b[0m result \u001b[39m=\u001b[39m original_function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1069\u001b[0m end_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m   1070\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m kwargs[\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/main.py:3084\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   3081\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[1;32m   3082\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   3083\u001b[0m     \u001b[39m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 3084\u001b[0m     \u001b[39mraise\u001b[39;00m exception_type(\n\u001b[1;32m   3085\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   3086\u001b[0m         custom_llm_provider\u001b[39m=\u001b[39;49mcustom_llm_provider,\n\u001b[1;32m   3087\u001b[0m         original_exception\u001b[39m=\u001b[39;49me,\n\u001b[1;32m   3088\u001b[0m         completion_kwargs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   3089\u001b[0m         extra_kwargs\u001b[39m=\u001b[39;49mkwargs,\n\u001b[1;32m   3090\u001b[0m     )\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2202\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2200\u001b[0m \u001b[39mif\u001b[39;00m exception_mapping_worked:\n\u001b[1;32m   2201\u001b[0m     \u001b[39msetattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mlitellm_response_headers\u001b[39m\u001b[39m\"\u001b[39m, litellm_response_headers)\n\u001b[0;32m-> 2202\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   2203\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2204\u001b[0m     \u001b[39mfor\u001b[39;00m error_type \u001b[39min\u001b[39;00m litellm\u001b[39m.\u001b[39mLITELLM_EXCEPTION_TYPES:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:417\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39melif\u001b[39;00m original_exception\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m422\u001b[39m:\n\u001b[1;32m    416\u001b[0m     exception_mapping_worked \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     \u001b[39mraise\u001b[39;00m BadRequestError(\n\u001b[1;32m    418\u001b[0m         message\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mexception_provider\u001b[39m}\u001b[39;00m\u001b[39m - \u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    419\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    420\u001b[0m         llm_provider\u001b[39m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m    421\u001b[0m         response\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(original_exception, \u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    422\u001b[0m         litellm_debug_info\u001b[39m=\u001b[39mextra_information,\n\u001b[1;32m    423\u001b[0m     )\n\u001b[1;32m    424\u001b[0m \u001b[39melif\u001b[39;00m original_exception\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m429\u001b[39m:\n\u001b[1;32m    425\u001b[0m     exception_mapping_worked \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: litellm.BadRequestError: OpenAIException - Failed to deserialize the JSON body into the target type: response_format.type: unknown variant `json_schema`, expected one of `json`, `json_object`, `regex` at line 1 column 656"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Ankara is the capital of Turkey.\n",
    "Claude Shannon is the father of information theory.\n",
    "\"\"\".strip()\n",
    "response = lm(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are an entity relation extraction model. You will be given a text and you will need to extract the entities and relations from the text.\\n\\nJSON Schema:\\n{schema}\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ],\n",
    "    response_format={ \"type\": \"json_schema\", \"json_schema\": schema , \"strict\": True }\n",
    ")\n",
    "\n",
    "try:\n",
    "    output = EntityRelationExtraction.model_validate_json(response[0])\n",
    "    print(repr(output))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: Hosted_vllmException - Failed to deserialize the JSON body into the target type: response_format.type: unknown variant `json_schema`, expected one of `json`, `json_object`, `regex` at line 1 column 656",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnprocessableEntityError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:691\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 691\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    692\u001b[0m \u001b[39m# e.message\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:653\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    641\u001b[0m logging_obj\u001b[39m.\u001b[39mpre_call(\n\u001b[1;32m    642\u001b[0m     \u001b[39minput\u001b[39m\u001b[39m=\u001b[39mmessages,\n\u001b[1;32m    643\u001b[0m     api_key\u001b[39m=\u001b[39mopenai_client\u001b[39m.\u001b[39mapi_key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m     },\n\u001b[1;32m    650\u001b[0m )\n\u001b[1;32m    652\u001b[0m headers, response \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 653\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_sync_openai_chat_completion_request(\n\u001b[1;32m    654\u001b[0m         openai_client\u001b[39m=\u001b[39;49mopenai_client,\n\u001b[1;32m    655\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    656\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    657\u001b[0m         logging_obj\u001b[39m=\u001b[39;49mlogging_obj,\n\u001b[1;32m    658\u001b[0m     )\n\u001b[1;32m    659\u001b[0m )\n\u001b[1;32m    661\u001b[0m logging_obj\u001b[39m.\u001b[39mmodel_call_details[\u001b[39m\"\u001b[39m\u001b[39mresponse_headers\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m headers\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_utils.py:145\u001b[0m, in \u001b[0;36mtrack_llm_api_timing.<locals>.decorator.<locals>.sync_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    146\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:472\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[0;34m(self, openai_client, data, timeout, logging_obj)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 472\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:454\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[0;34m(self, openai_client, data, timeout, logging_obj)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     raw_response \u001b[39m=\u001b[39m openai_client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mwith_raw_response\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    455\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdata, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    456\u001b[0m     )\n\u001b[1;32m    458\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(raw_response, \u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mextra_headers\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[39mreturn\u001b[39;00m cast(LegacyAPIResponse[R], func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:879\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    878\u001b[0m validate_response_format(response_format)\n\u001b[0;32m--> 879\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    880\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    881\u001b[0m     body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    882\u001b[0m         {\n\u001b[1;32m    883\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    884\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    885\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m\"\u001b[39;49m: audio,\n\u001b[1;32m    886\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    887\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    888\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    889\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    890\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    891\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmax_completion_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_completion_tokens,\n\u001b[1;32m    892\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    893\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m: metadata,\n\u001b[1;32m    894\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmodalities\u001b[39;49m\u001b[39m\"\u001b[39;49m: modalities,\n\u001b[1;32m    895\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    896\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mparallel_tool_calls\u001b[39;49m\u001b[39m\"\u001b[39;49m: parallel_tool_calls,\n\u001b[1;32m    897\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mprediction\u001b[39;49m\u001b[39m\"\u001b[39;49m: prediction,\n\u001b[1;32m    898\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    899\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mreasoning_effort\u001b[39;49m\u001b[39m\"\u001b[39;49m: reasoning_effort,\n\u001b[1;32m    900\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    901\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    902\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mservice_tier\u001b[39;49m\u001b[39m\"\u001b[39;49m: service_tier,\n\u001b[1;32m    903\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    904\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstore\u001b[39;49m\u001b[39m\"\u001b[39;49m: store,\n\u001b[1;32m    905\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    906\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstream_options\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream_options,\n\u001b[1;32m    907\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    908\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    909\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    910\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[1;32m    911\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    912\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    913\u001b[0m         },\n\u001b[1;32m    914\u001b[0m         completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    915\u001b[0m     ),\n\u001b[1;32m    916\u001b[0m     options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    917\u001b[0m         extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    918\u001b[0m     ),\n\u001b[1;32m    919\u001b[0m     cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    920\u001b[0m     stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    921\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    922\u001b[0m )\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/_base_client.py:1290\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1287\u001b[0m opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1288\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1289\u001b[0m )\n\u001b[0;32m-> 1290\u001b[0m \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/_base_client.py:967\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    965\u001b[0m     retries_taken \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 967\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    968\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    969\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    970\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    971\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    972\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m    973\u001b[0m )\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/openai/_base_client.py:1071\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1071\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[1;32m   1074\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1075\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1079\u001b[0m     retries_taken\u001b[39m=\u001b[39mretries_taken,\n\u001b[1;32m   1080\u001b[0m )\n",
      "\u001b[0;31mUnprocessableEntityError\u001b[0m: Failed to deserialize the JSON body into the target type: response_format.type: unknown variant `json_schema`, expected one of `json`, `json_object`, `regex` at line 1 column 656",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/main.py:1723\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     logging\u001b[39m.\u001b[39mpost_call(\n\u001b[1;32m   1718\u001b[0m         \u001b[39minput\u001b[39m\u001b[39m=\u001b[39mmessages,\n\u001b[1;32m   1719\u001b[0m         api_key\u001b[39m=\u001b[39mapi_key,\n\u001b[1;32m   1720\u001b[0m         original_response\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(e),\n\u001b[1;32m   1721\u001b[0m         additional_args\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m: headers},\n\u001b[1;32m   1722\u001b[0m     )\n\u001b[0;32m-> 1723\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   1725\u001b[0m \u001b[39mif\u001b[39;00m optional_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1726\u001b[0m     \u001b[39m## LOGGING\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/main.py:1696\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1695\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1696\u001b[0m     response \u001b[39m=\u001b[39m openai_chat_completions\u001b[39m.\u001b[39;49mcompletion(\n\u001b[1;32m   1697\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1698\u001b[0m         messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m   1699\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1700\u001b[0m         model_response\u001b[39m=\u001b[39;49mmodel_response,\n\u001b[1;32m   1701\u001b[0m         print_verbose\u001b[39m=\u001b[39;49mprint_verbose,\n\u001b[1;32m   1702\u001b[0m         api_key\u001b[39m=\u001b[39;49mapi_key,\n\u001b[1;32m   1703\u001b[0m         api_base\u001b[39m=\u001b[39;49mapi_base,\n\u001b[1;32m   1704\u001b[0m         acompletion\u001b[39m=\u001b[39;49macompletion,\n\u001b[1;32m   1705\u001b[0m         logging_obj\u001b[39m=\u001b[39;49mlogging,\n\u001b[1;32m   1706\u001b[0m         optional_params\u001b[39m=\u001b[39;49moptional_params,\n\u001b[1;32m   1707\u001b[0m         litellm_params\u001b[39m=\u001b[39;49mlitellm_params,\n\u001b[1;32m   1708\u001b[0m         logger_fn\u001b[39m=\u001b[39;49mlogger_fn,\n\u001b[1;32m   1709\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   1710\u001b[0m         custom_prompt_dict\u001b[39m=\u001b[39;49mcustom_prompt_dict,\n\u001b[1;32m   1711\u001b[0m         client\u001b[39m=\u001b[39;49mclient,  \u001b[39m# pass AsyncOpenAI, OpenAI client\u001b[39;49;00m\n\u001b[1;32m   1712\u001b[0m         organization\u001b[39m=\u001b[39;49morganization,\n\u001b[1;32m   1713\u001b[0m         custom_llm_provider\u001b[39m=\u001b[39;49mcustom_llm_provider,\n\u001b[1;32m   1714\u001b[0m     )\n\u001b[1;32m   1715\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1716\u001b[0m     \u001b[39m## LOGGING - log the original exception returned\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:736\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    735\u001b[0m     error_headers \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(error_response, \u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 736\u001b[0m \u001b[39mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    737\u001b[0m     status_code\u001b[39m=\u001b[39mstatus_code, message\u001b[39m=\u001b[39merror_text, headers\u001b[39m=\u001b[39merror_headers\n\u001b[1;32m    738\u001b[0m )\n",
      "\u001b[0;31mOpenAIError\u001b[0m: Failed to deserialize the JSON body into the target type: response_format.type: unknown variant `json_schema`, expected one of `json`, `json_object`, `regex` at line 1 column 656",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mAnkara is the capital of Turkey.\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mClaude Shannon is the father of information theory.\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m response \u001b[39m=\u001b[39m lm(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         {\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mYou are an entity relation extraction model. You will be given a text and you will need to extract the entities and relations from the text.\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mJSON Schema:\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m{\u001b[39;49;00mschema\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         },\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: text},\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     ],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     response_format\u001b[39m=\u001b[39;49mEntityRelationExtraction,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-llm-adaptation/nbs/erx/debug-llm-compat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     output \u001b[39m=\u001b[39m EntityRelationExtraction\u001b[39m.\u001b[39mmodel_validate_json(response[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/utils/callback.py:234\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39m# If no callbacks are provided, just call the function\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(instance, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    236\u001b[0m \u001b[39m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m call_id \u001b[39m=\u001b[39m uuid\u001b[39m.\u001b[39muuid4()\u001b[39m.\u001b[39mhex\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/clients/lm.py:119\u001b[0m, in \u001b[0;36mLM.__call__\u001b[0;34m(self, prompt, messages, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     completion \u001b[39m=\u001b[39m litellm_completion \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mchat\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m litellm_text_completion\n\u001b[0;32m--> 119\u001b[0m     response \u001b[39m=\u001b[39m completion(\n\u001b[1;32m    120\u001b[0m         request\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, messages\u001b[39m=\u001b[39;49mmessages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs),\n\u001b[1;32m    121\u001b[0m         num_retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_retries,\n\u001b[1;32m    122\u001b[0m         \u001b[39m# only leverage LiteLLM cache in this case\u001b[39;49;00m\n\u001b[1;32m    123\u001b[0m         cache\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mno-cache\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mnot\u001b[39;49;00m cache, \u001b[39m\"\u001b[39;49m\u001b[39mno-store\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mnot\u001b[39;49;00m cache},\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    127\u001b[0m     outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    128\u001b[0m         {\n\u001b[1;32m    129\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: c\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(c, \u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m c[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    133\u001b[0m     ]\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/dspy/clients/lm.py:350\u001b[0m, in \u001b[0;36mlitellm_completion\u001b[0;34m(request, num_retries, cache)\u001b[0m\n\u001b[1;32m    348\u001b[0m stream \u001b[39m=\u001b[39m dspy\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39msend_stream\n\u001b[1;32m    349\u001b[0m \u001b[39mif\u001b[39;00m stream \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 350\u001b[0m     \u001b[39mreturn\u001b[39;00m litellm\u001b[39m.\u001b[39;49mcompletion(\n\u001b[1;32m    351\u001b[0m         cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m    352\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mretry_kwargs,\n\u001b[1;32m    353\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest,\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    356\u001b[0m \u001b[39m# The stream is already opened, and will be closed by the caller.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m stream \u001b[39m=\u001b[39m cast(MemoryObjectSendStream, stream)\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/utils.py:1190\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39mif\u001b[39;00m logging_obj:\n\u001b[1;32m   1187\u001b[0m     logging_obj\u001b[39m.\u001b[39mfailure_handler(\n\u001b[1;32m   1188\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1189\u001b[0m     )  \u001b[39m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/utils.py:1068\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         print_verbose(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError while checking max token limit: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1067\u001b[0m \u001b[39m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1068\u001b[0m result \u001b[39m=\u001b[39m original_function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1069\u001b[0m end_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m   1070\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m kwargs[\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/main.py:3084\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   3081\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[1;32m   3082\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   3083\u001b[0m     \u001b[39m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 3084\u001b[0m     \u001b[39mraise\u001b[39;00m exception_type(\n\u001b[1;32m   3085\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   3086\u001b[0m         custom_llm_provider\u001b[39m=\u001b[39;49mcustom_llm_provider,\n\u001b[1;32m   3087\u001b[0m         original_exception\u001b[39m=\u001b[39;49me,\n\u001b[1;32m   3088\u001b[0m         completion_kwargs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   3089\u001b[0m         extra_kwargs\u001b[39m=\u001b[39;49mkwargs,\n\u001b[1;32m   3090\u001b[0m     )\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2202\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2200\u001b[0m \u001b[39mif\u001b[39;00m exception_mapping_worked:\n\u001b[1;32m   2201\u001b[0m     \u001b[39msetattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mlitellm_response_headers\u001b[39m\u001b[39m\"\u001b[39m, litellm_response_headers)\n\u001b[0;32m-> 2202\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   2203\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2204\u001b[0m     \u001b[39mfor\u001b[39;00m error_type \u001b[39min\u001b[39;00m litellm\u001b[39m.\u001b[39mLITELLM_EXCEPTION_TYPES:\n",
      "File \u001b[0;32m~/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:417\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39melif\u001b[39;00m original_exception\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m422\u001b[39m:\n\u001b[1;32m    416\u001b[0m     exception_mapping_worked \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     \u001b[39mraise\u001b[39;00m BadRequestError(\n\u001b[1;32m    418\u001b[0m         message\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mexception_provider\u001b[39m}\u001b[39;00m\u001b[39m - \u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    419\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    420\u001b[0m         llm_provider\u001b[39m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m    421\u001b[0m         response\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(original_exception, \u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    422\u001b[0m         litellm_debug_info\u001b[39m=\u001b[39mextra_information,\n\u001b[1;32m    423\u001b[0m     )\n\u001b[1;32m    424\u001b[0m \u001b[39melif\u001b[39;00m original_exception\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m429\u001b[39m:\n\u001b[1;32m    425\u001b[0m     exception_mapping_worked \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: litellm.BadRequestError: Hosted_vllmException - Failed to deserialize the JSON body into the target type: response_format.type: unknown variant `json_schema`, expected one of `json`, `json_object`, `regex` at line 1 column 656"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Ankara is the capital of Turkey.\n",
    "Claude Shannon is the father of information theory.\n",
    "\"\"\".strip()\n",
    "response = lm(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are an entity relation extraction model. You will be given a text and you will need to extract the entities and relations from the text.\\n\\nJSON Schema:\\n{schema}\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ],\n",
    "    response_format=EntityRelationExtraction,\n",
    ")\n",
    "\n",
    "try:\n",
    "    output = EntityRelationExtraction.model_validate_json(response[0])\n",
    "    print(output)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baris/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/pydantic/main.py:426: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `str` but got `dict` with value `{'triples': ['Ankara', 'i..., 'information theory']}` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = \"\"\"\n",
    "# Ankara is the capital of Turkey.\n",
    "# Claude Shannon is the father of information theory.\n",
    "# \"\"\".strip()\n",
    "# response = lm(\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": f\"You are an entity relation extraction model. You will be given a text and you will need to extract the entities and relations from the text.\\n\\nJSON Schema:\\n{schema}\",\n",
    "#         },\n",
    "#         {\"role\": \"user\", \"content\": text},\n",
    "#     ],\n",
    "#     tools= [\n",
    "#         {\n",
    "#             \"type\": \"function\",\n",
    "#             \"function\": {\n",
    "#                 \"name\": \"save\",\n",
    "#                 \"description\": \"Save extracted triples\",\n",
    "#                 \"parameters\": schema,\n",
    "#             }\n",
    "#         }\n",
    "#     ],\n",
    "#     tool_choice=\"save\"\n",
    "# )\n",
    "\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
