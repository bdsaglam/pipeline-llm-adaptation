{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import dspy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import weave\n",
    "from datasets import load_dataset\n",
    "from dspy.evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed % (2**32 - 1))\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def configure_lm(model, temperature):\n",
    "    lm = dspy.LM(\n",
    "        \"openai/\" + model,\n",
    "        temperature=temperature,\n",
    "        cache=False,\n",
    "        api_base=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    )\n",
    "    dspy.configure(lm=lm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weave.init(project_name=\"llm-adaptation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(89)\n",
    "\n",
    "configure_lm('llama-3-8b', 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "def compute_generalized_scores(\n",
    "    pred_triples: list[str], \n",
    "    reference_triples: list[str], \n",
    "    match_function: Callable[[str, str], bool]\n",
    "):\n",
    "    \"\"\"Compute precision, recall, and F1-score using a customizable match function.\"\"\"\n",
    "\n",
    "    pred_set = set(pred_triples)\n",
    "    reference_set = set(reference_triples)\n",
    "\n",
    "    if not pred_set and not reference_set:\n",
    "        return {\"precision\": 1.0, \"recall\": 1.0, \"f1\": 1.0}\n",
    "\n",
    "    if not pred_set or not reference_set:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "\n",
    "    # Count true positives using the provided match function\n",
    "    true_positives = sum(any(match_function(pred, ref) for ref in reference_set) for pred in pred_set)\n",
    "    \n",
    "    precision = true_positives / len(pred_set)\n",
    "    recall = true_positives / len(reference_set)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baris/repos/pipeline-llm-adaptation/.venv/lib/python3.11/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "def fuzzy_match(pred: str, ref: str) -> bool:\n",
    "    return fuzz.ratio(pred, ref) > 80\n",
    "\n",
    "def compute_scores(pred_triples: list[str], reference_triples: list[str]):\n",
    "    exact_scores = {f\"exact.{k}\": v for k, v in  compute_generalized_scores(pred_triples, reference_triples, lambda x, y: x == y).items()}\n",
    "    fuzzy_scores = {f\"fuzzy.{k}\": v for k, v in  compute_generalized_scores(pred_triples, reference_triples, fuzzy_match).items()}\n",
    "    return {**exact_scores, **fuzzy_scores}\n",
    "\n",
    "def parse_triples(triples_str: str):\n",
    "    return [triple.strip() for triple in triples_str.split('\\n') if triple.strip()]\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def evaluate_triples(example, pred, trace=None):\n",
    "    return compute_scores(parse_triples(pred.triples_str), example.triples)['fuzzy.f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def dynamic_import(module, name):\n",
    "    import importlib\n",
    "\n",
    "    return getattr(importlib.import_module(module), name)\n",
    "\n",
    "def make_optimizer(optimizer_config: dict):\n",
    "    cls = dynamic_import(\"dspy.teleprompt\", optimizer_config[\"class\"])\n",
    "    kwargs = deepcopy(optimizer_config[\"params\"])\n",
    "    if optimizer_config[\"with_metric\"]:\n",
    "        kwargs[\"metric\"] = evaluate_triples\n",
    "    return cls(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityRelationExtraction(dspy.Signature):\n",
    "    \"\"\"Extract `subject | predicate | object` triples from text.\"\"\"\n",
    "\n",
    "    text: str = dspy.InputField()\n",
    "    triples_str: str = dspy.OutputField(\n",
    "        desc='The triples extracted from the text. Each triple should be in the format \"subject | predicate | object\". Triples should be separated by newlines.'\n",
    "    )\n",
    "\n",
    "\n",
    "def make_program():\n",
    "    return dspy.Predict(EntityRelationExtraction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ankara is the capital of Turkey.\n",
      "Claude Shannon is the father of information theory.\n",
      "Prediction(\n",
      "    triples_str='Ankara | is the capital of | Turkey\\nClaude Shannon | is the father of | information theory'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Ankara is the capital of Turkey.\n",
    "Claude Shannon is the father of information theory.\n",
    "\"\"\".strip()\n",
    "\n",
    "response = make_program()(text=text)\n",
    "print(text)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset_path: str,\n",
    "    dataset_name: str,\n",
    "    dataset_split: str,\n",
    "    model: str,\n",
    "    temperature: float,\n",
    "    optimizer_config: dict,\n",
    "    out: str,\n",
    "):\n",
    "    # Set up LM\n",
    "    configure_lm(model, temperature)\n",
    "\n",
    "    # Load dataset\n",
    "    ds = load_dataset(dataset_path, dataset_name, split=dataset_split)\n",
    "    examples = [dspy.Example(text=x[\"text\"], triples=x[\"triples\"]).with_inputs(\"text\") for x in ds]\n",
    "\n",
    "    # Create program\n",
    "    program = make_program()\n",
    "\n",
    "    # Create and run optimizer\n",
    "    optimizer = make_optimizer(optimizer_config)\n",
    "    compile_params = optimizer_config.get(\"compile_params\", {})\n",
    "    trained_program = optimizer.compile(program, trainset=examples, **compile_params)\n",
    "\n",
    "    # Save trained program\n",
    "    Path(out).parent.mkdir(parents=True, exist_ok=True)\n",
    "    trained_program.save(out)\n",
    "    return trained_program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    dataset_path: str,\n",
    "    dataset_name: str,\n",
    "    dataset_split: str,\n",
    "    model: str,\n",
    "    temperature: float,\n",
    "    out_dir: str,\n",
    "    load_from: str | None = None,\n",
    "):\n",
    "    # Set up LM\n",
    "    configure_lm(model, temperature)\n",
    "\n",
    "    # Load dataset\n",
    "    ds = load_dataset(dataset_path, dataset_name, split=dataset_split)\n",
    "    examples = [dspy.Example(text=x[\"text\"], triples=x[\"triples\"]).with_inputs(\"text\") for x in ds]\n",
    "\n",
    "    # Load program\n",
    "    program = make_program()\n",
    "    if load_from:\n",
    "        program.load(load_from)\n",
    "\n",
    "    # Evaluate\n",
    "    evaluator = Evaluate(\n",
    "        metric=evaluate_triples,\n",
    "        devset=examples,\n",
    "        num_threads=16,\n",
    "        display_progress=True,\n",
    "        return_outputs=True,\n",
    "    )\n",
    "    _, results = evaluator(program)\n",
    "\n",
    "    # Save results\n",
    "    out_path = Path(out_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Process and save detailed results\n",
    "    processed_results = []\n",
    "    for example, pred, score in results:\n",
    "        result = {\n",
    "            \"text\": example.text,\n",
    "            \"triples\": example.triples,\n",
    "            \"predicted_triples\": parse_triples(pred.triples_str),\n",
    "            **compute_scores(parse_triples(pred.triples_str), example.triples),\n",
    "        }\n",
    "        processed_results.append(result)\n",
    "    \n",
    "    result_df = pd.DataFrame(processed_results)\n",
    "    result_df.to_json(out_path / \"results.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "    # Save aggregate scores\n",
    "    scores = result_df[[\"exact.precision\", \"exact.recall\", \"exact.f1\", \"fuzzy.precision\", \"fuzzy.recall\", \"fuzzy.f1\"]].mean().to_dict()\n",
    "    with open(out_path / \"scores.json\", \"w\") as f:\n",
    "        json.dump(scores, f, indent=2)\n",
    "\n",
    "    return results, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"bdsaglam/web_nlg-erx-concat\"\n",
    "dataset_name = \"release_v3.0_en\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before prompt optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "results, scores = evaluate(\n",
    "    dataset_path=dataset_path,\n",
    "    dataset_name=dataset_name,\n",
    "    dataset_split=\"dev[:100]\",\n",
    "    model=\"llama-3-8b\",\n",
    "    temperature=0.0,\n",
    "    out_dir=\"../tmp/erx/dspy/before\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact.precision': 0.011607142857142856,\n",
       " 'exact.recall': 0.012751696832579184,\n",
       " 'exact.f1': 0.012068311648983919,\n",
       " 'fuzzy.precision': 0.34766847244117066,\n",
       " 'fuzzy.recall': 0.3378176513648623,\n",
       " 'fuzzy.f1': 0.33974898258125846}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's optimize the program with DSPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 8 traces per predictor.\n",
      "Will attempt to bootstrap 16 candidate sets.\n",
      "Average Metric: 31.60 / 100 (31.6%): 100%|██████████| 100/100 [00:17<00:00,  5.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:25:14 INFO dspy.evaluate.evaluate: Average Metric: 31.600189246922703 / 100 (31.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 31.6 for seed -3\n",
      "Scores so far: [31.6]\n",
      "Best score so far: 31.6\n",
      "Average Metric: 32.02 / 100 (32.0%): 100%|██████████| 100/100 [00:17<00:00,  5.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:25:31 INFO dspy.evaluate.evaluate: Average Metric: 32.01976466366833 / 100 (32.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 32.02 for seed -2\n",
      "Scores so far: [31.6, 32.02]\n",
      "Best score so far: 32.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:14<02:41,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 16 rounds, amounting to 9 attempts.\n",
      "Average Metric: 35.02 / 100 (35.0%): 100%|██████████| 100/100 [00:22<00:00,  4.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:26:08 INFO dspy.evaluate.evaluate: Average Metric: 35.021371833660346 / 100 (35.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 35.02 for seed -1\n",
      "Scores so far: [31.6, 32.02, 35.02]\n",
      "Best score so far: 35.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:11<02:27,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 7 full traces after 7 examples for up to 16 rounds, amounting to 8 attempts.\n",
      "Average Metric: 0.00 / 97 (0.0%):  96%|█████████▌| 96/100 [00:05<00:00, 29.66it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:26:28 ERROR dspy.utils.parallelizer: Error processing item Example({'text': \"Ann Arbor forms part of Washtenaw County in Michigan, United States. Detroit is the largest city in Michigan, where the capital is Lansing.\\nThe TV character Bananaman was created by John Geering. The programme was broadcast by STV and starred Graeme Garden. Bananaman was first shown on 10th March 1983 and last aired on 15th April 1986.\\nAkeem Adams' former clubs include Ferencvarosi TC and Palo Seco based United Petrotrin FC.\\nVehicles that are related are the Alfa Romeo 164 (made in Italy) and Lancia Thema. The latter is related to the Saab 9000.\\nThe AIDAstella was built by the German based company Meyer Werft. It is operated by AIDA Cruises and owned by Costa Crociere, a subsidiary of Carnival Corporation & Plc.\\nAleksandr Prudnikov belonged to FC Spartak Moscow, which plays at Otkrytiye Arena. He has played fro FC Terek Grozny and plays for FC Amkar Perm, which is managed by Gadzhi Gadzhiyev.\", 'triples': ['Ann Arbor, Michigan | is part of | Washtenaw County, Michigan', 'Michigan | largest city | Detroit', 'Michigan | country | United States', 'Ann Arbor, Michigan | is part of | Michigan', 'Michigan | capital | Lansing, Michigan', 'Bananaman | creator | John Geering', 'Bananaman | starring | Graeme Garden', 'Bananaman | first aired | 1983-10-03', 'Bananaman | last aired | 1986-04-15', 'Bananaman | broadcasted by | STV', 'Akeem Adams | club | Ferencvárosi TC', 'Akeem Adams | club | United Petrotrin F.C.', 'United Petrotrin F.C. | ground | Palo Seco', 'Lancia Thema | related mean of transportation | Saab 9000', 'Alfa Romeo 164 | assembly | Italy', 'Alfa Romeo 164 | related mean of transportation | Lancia Thema', 'Costa Crociere | parent company | Carnival Corporation & plc', 'AIDAstella | operator | AIDA Cruises', 'Meyer Werft | location | Germany', 'AIDAstella | builder | Meyer Werft', 'AIDAstella | owner | Costa Crociere', 'Aleksandr Prudnikov | club | FC Amkar Perm', 'FC Spartak Moscow | ground | Otkrytiye Arena', 'Aleksandr Prudnikov | youthclub | FC Spartak Moscow', 'Aleksandr Prudnikov | club | FC Terek Grozny', 'FC Amkar Perm | manager | Gadzhi Gadzhiyev']}) (input_keys={'text'}): litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': {'message': 'litellm.BadRequestError: OpenAIException - Failed to deserialize the JSON body into the target type: response_format: missing field `value` at line 1 column 10142\\nReceived Model Group=llama-3-8b\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '400'}}. Set `provide_traceback=True` to see the stack trace.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 99 (0.0%): 100%|██████████| 100/100 [00:13<00:00,  7.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:26:33 INFO dspy.evaluate.evaluate: Average Metric: 0.0 / 100 (0.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [31.6, 32.02, 35.02, 0.0]\n",
      "Best score so far: 35.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:04<02:12,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 3 examples for up to 16 rounds, amounting to 3 attempts.\n",
      "Average Metric: 32.74 / 100 (32.7%): 100%|██████████| 100/100 [00:19<00:00,  5.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:26:57 INFO dspy.evaluate.evaluate: Average Metric: 32.74268601162082 / 100 (32.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [31.6, 32.02, 35.02, 0.0, 32.74]\n",
      "Best score so far: 35.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:03<06:07,  3.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 16 rounds, amounting to 4 attempts.\n",
      "Average Metric: 38.90 / 100 (38.9%): 100%|██████████| 100/100 [00:18<00:00,  5.52it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:27:19 INFO dspy.evaluate.evaluate: Average Metric: 38.89527223804535 / 100 (38.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 38.9 for seed 2\n",
      "Scores so far: [31.6, 32.02, 35.02, 0.0, 32.74, 38.9]\n",
      "Best score so far: 38.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:20<06:22,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples for up to 16 rounds, amounting to 20 attempts.\n",
      "Average Metric: 42.75 / 100 (42.8%): 100%|██████████| 100/100 [00:19<00:00,  5.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:28:00 INFO dspy.evaluate.evaluate: Average Metric: 42.75407322418495 / 100 (42.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 42.75 for seed 3\n",
      "Scores so far: [31.6, 32.02, 35.02, 0.0, 32.74, 38.9, 42.75]\n",
      "Best score so far: 42.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:09<03:43,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 4 examples for up to 16 rounds, amounting to 4 attempts.\n",
      "Average Metric: 31.46 / 100 (31.5%): 100%|██████████| 100/100 [00:21<00:00,  4.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:28:31 INFO dspy.evaluate.evaluate: Average Metric: 31.457793875105306 / 100 (31.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [31.6, 32.02, 35.02, 0.0, 32.74, 38.9, 42.75, 31.46]\n",
      "Best score so far: 42.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:08<02:45,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 5 full traces after 5 examples for up to 16 rounds, amounting to 5 attempts.\n",
      "Average Metric: 33.81 / 100 (33.8%): 100%|██████████| 100/100 [00:20<00:00,  4.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:29:01 INFO dspy.evaluate.evaluate: Average Metric: 33.812326256486195 / 100 (33.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [31.6, 32.02, 35.02, 0.0, 32.74, 38.9, 42.75, 31.46, 33.81]\n",
      "Best score so far: 42.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:15<12:18,  7.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 16 rounds, amounting to 6 attempts.\n",
      "Average Metric: 35.01 / 100 (35.0%): 100%|██████████| 100/100 [00:17<00:00,  5.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:29:35 INFO dspy.evaluate.evaluate: Average Metric: 35.00951853133457 / 100 (35.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [31.6, 32.02, 35.02, 0.0, 32.74, 38.9, 42.75, 31.46, 33.81, 35.01]\n",
      "Best score so far: 42.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:10<02:45,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 6 full traces after 6 examples for up to 16 rounds, amounting to 6 attempts.\n",
      "Average Metric: 34.30 / 100 (34.3%): 100%|██████████| 100/100 [00:22<00:00,  4.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:30:09 INFO dspy.evaluate.evaluate: Average Metric: 34.30256938145835 / 100 (34.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [31.6, 32.02, 35.02, 0.0, 32.74, 38.9, 42.75, 31.46, 33.81, 35.01, 34.3]\n",
      "Best score so far: 42.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:14<05:42,  3.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 4 examples for up to 16 rounds, amounting to 16 attempts.\n",
      "Average Metric: 36.15 / 100 (36.2%): 100%|██████████| 100/100 [00:20<00:00,  4.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:30:44 INFO dspy.evaluate.evaluate: Average Metric: 36.153882269280516 / 100 (36.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [31.6, 32.02, 35.02, 0.0, 32.74, 38.9, 42.75, 31.46, 33.81, 35.01, 34.3, 36.15]\n",
      "Best score so far: 42.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:14<02:51,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 16 rounds, amounting to 8 attempts.\n",
      "Average Metric: 31.24 / 100 (31.2%): 100%|██████████| 100/100 [00:23<00:00,  4.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:31:23 INFO dspy.evaluate.evaluate: Average Metric: 31.23734205861276 / 100 (31.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [31.6, 32.02, 35.02, 0.0, 32.74, 38.9, 42.75, 31.46, 33.81, 35.01, 34.3, 36.15, 31.24]\n",
      "Best score so far: 42.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:03<05:35,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 16 rounds, amounting to 3 attempts.\n",
      "Average Metric: 32.13 / 100 (32.1%): 100%|██████████| 100/100 [00:18<00:00,  5.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:31:45 INFO dspy.evaluate.evaluate: Average Metric: 32.13415569360357 / 100 (32.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [31.6, 32.02, 35.02, 0.0, 32.74, 38.9, 42.75, 31.46, 33.81, 35.01, 34.3, 36.15, 31.24, 32.13]\n",
      "Best score so far: 42.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:16<03:06,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 16 rounds, amounting to 8 attempts.\n",
      "Average Metric: 35.68 / 100 (35.7%): 100%|██████████| 100/100 [00:23<00:00,  4.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:32:26 INFO dspy.evaluate.evaluate: Average Metric: 35.67776527737274 / 100 (35.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [31.6, 32.02, 35.02, 0.0, 32.74, 38.9, 42.75, 31.46, 33.81, 35.01, 34.3, 36.15, 31.24, 32.13, 35.68]\n",
      "Best score so far: 42.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:15<02:59,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 16 rounds, amounting to 8 attempts.\n",
      "Average Metric: 36.20 / 100 (36.2%): 100%|██████████| 100/100 [00:23<00:00,  4.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:33:06 INFO dspy.evaluate.evaluate: Average Metric: 36.1985664905158 / 100 (36.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [31.6, 32.02, 35.02, 0.0, 32.74, 38.9, 42.75, 31.46, 33.81, 35.01, 34.3, 36.15, 31.24, 32.13, 35.68, 36.2]\n",
      "Best score so far: 42.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:07<02:15,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 5 full traces after 5 examples for up to 16 rounds, amounting to 8 attempts.\n",
      "Average Metric: 42.92 / 100 (42.9%): 100%|██████████| 100/100 [00:18<00:00,  5.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:33:33 INFO dspy.evaluate.evaluate: Average Metric: 42.917826619044305 / 100 (42.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 42.92 for seed 13\n",
      "Scores so far: [31.6, 32.02, 35.02, 0.0, 32.74, 38.9, 42.75, 31.46, 33.81, 35.01, 34.3, 36.15, 31.24, 32.13, 35.68, 36.2, 42.92]\n",
      "Best score so far: 42.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:09<07:49,  4.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 16 rounds, amounting to 4 attempts.\n",
      "Average Metric: 36.60 / 100 (36.6%): 100%|██████████| 100/100 [00:20<00:00,  4.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:34:03 INFO dspy.evaluate.evaluate: Average Metric: 36.59968675831522 / 100 (36.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [31.6, 32.02, 35.02, 0.0, 32.74, 38.9, 42.75, 31.46, 33.81, 35.01, 34.3, 36.15, 31.24, 32.13, 35.68, 36.2, 42.92, 36.6]\n",
      "Best score so far: 42.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:08<03:21,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 4 examples for up to 16 rounds, amounting to 6 attempts.\n",
      "Average Metric: 41.68 / 100 (41.7%): 100%|██████████| 100/100 [00:19<00:00,  5.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:34:31 INFO dspy.evaluate.evaluate: Average Metric: 41.68390665799123 / 100 (41.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [31.6, 32.02, 35.02, 0.0, 32.74, 38.9, 42.75, 31.46, 33.81, 35.01, 34.3, 36.15, 31.24, 32.13, 35.68, 36.2, 42.92, 36.6, 41.68]\n",
      "Best score so far: 42.92\n",
      "19 candidate programs found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Predict(EntityRelationExtraction(text -> triples_str\n",
       "    instructions='Extract `subject | predicate | object` triples from text.'\n",
       "    text = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Text:', 'desc': '${text}'})\n",
       "    triples_str = Field(annotation=str required=True json_schema_extra={'desc': 'The triples extracted from the text. Each triple should be in the format \"subject | predicate | object\". Triples should be separated by newlines.', '__dspy_field_type': 'output', 'prefix': 'Triples Str:'})\n",
       "))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer_config = {\n",
    "    \"class\": \"BootstrapFewShotWithRandomSearch\",\n",
    "    \"params\": {\n",
    "        \"max_bootstrapped_demos\": 8,\n",
    "        \"max_labeled_demos\": 8,\n",
    "        \"max_rounds\": 16,\n",
    "        \"num_candidate_programs\": 16,\n",
    "        \"num_threads\": 16,\n",
    "        \"max_errors\": 10,\n",
    "        \"metric_threshold\": None,\n",
    "        \"stop_at_score\": None,\n",
    "    },\n",
    "    \"with_metric\": True,\n",
    "}\n",
    "\n",
    "train(\n",
    "    dataset_path=dataset_path,\n",
    "    dataset_name=dataset_name,\n",
    "    dataset_split=\"train[:100]\",\n",
    "    model=\"llama-3-8b\",\n",
    "    temperature=0.0,\n",
    "    optimizer_config=optimizer_config,\n",
    "    out=\"../tmp/erx/dspy/trained-program.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 39.40 / 100 (39.4%): 100%|██████████| 100/100 [00:18<00:00,  5.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/14 13:10:40 INFO dspy.evaluate.evaluate: Average Metric: 39.40046495393733 / 100 (39.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results, scores = evaluate(\n",
    "    dataset_path=dataset_path,\n",
    "    dataset_name=dataset_name,\n",
    "    dataset_split=\"dev[:100]\",\n",
    "    model=\"llama-3-8b\",\n",
    "    temperature=0.0,\n",
    "    out_dir=\"../tmp/erx/dspy/after\",\n",
    "    load_from=\"../tmp/erx/dspy/trained-program.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
